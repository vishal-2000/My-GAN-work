{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5851, 28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, d1=0.354, d2=0.705 g=0.015, a1=90, a2=39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2, d1=0.049, d2=0.868 g=0.005, a1=100, a2=0\n",
      ">3, d1=0.033, d2=0.867 g=0.005, a1=100, a2=0\n",
      ">4, d1=0.023, d2=0.814 g=0.005, a1=100, a2=0\n",
      ">5, d1=0.020, d2=0.755 g=0.005, a1=100, a2=0\n",
      ">6, d1=0.020, d2=0.707 g=0.006, a1=100, a2=18\n",
      ">7, d1=0.013, d2=0.667 g=0.004, a1=100, a2=89\n",
      ">8, d1=0.017, d2=0.628 g=0.004, a1=100, a2=100\n",
      ">9, d1=0.015, d2=0.582 g=0.004, a1=100, a2=100\n",
      ">10, d1=0.015, d2=0.545 g=0.003, a1=100, a2=100\n",
      ">11, d1=0.014, d2=0.483 g=0.003, a1=100, a2=100\n",
      ">12, d1=0.017, d2=0.415 g=0.003, a1=100, a2=100\n",
      ">13, d1=0.017, d2=0.360 g=0.003, a1=100, a2=100\n",
      ">14, d1=0.016, d2=0.317 g=0.003, a1=100, a2=100\n",
      ">15, d1=0.015, d2=0.267 g=0.002, a1=100, a2=100\n",
      ">16, d1=0.010, d2=0.222 g=0.002, a1=100, a2=100\n",
      ">17, d1=0.015, d2=0.184 g=0.002, a1=100, a2=100\n",
      ">18, d1=0.009, d2=0.154 g=0.002, a1=100, a2=100\n",
      ">19, d1=0.010, d2=0.131 g=0.002, a1=100, a2=100\n",
      ">20, d1=0.011, d2=0.111 g=0.002, a1=100, a2=100\n",
      ">21, d1=0.010, d2=0.092 g=0.002, a1=100, a2=100\n",
      ">22, d1=0.006, d2=0.082 g=0.001, a1=100, a2=100\n",
      ">23, d1=0.009, d2=0.067 g=0.002, a1=100, a2=100\n",
      ">24, d1=0.007, d2=0.058 g=0.001, a1=100, a2=100\n",
      ">25, d1=0.006, d2=0.051 g=0.001, a1=100, a2=100\n",
      ">26, d1=0.009, d2=0.044 g=0.001, a1=100, a2=100\n",
      ">27, d1=0.004, d2=0.038 g=0.001, a1=100, a2=100\n",
      ">28, d1=0.007, d2=0.034 g=0.001, a1=100, a2=100\n",
      ">29, d1=0.004, d2=0.030 g=0.001, a1=100, a2=100\n",
      ">30, d1=0.005, d2=0.027 g=0.001, a1=100, a2=100\n",
      ">31, d1=0.003, d2=0.024 g=0.001, a1=100, a2=100\n",
      ">32, d1=0.004, d2=0.022 g=0.001, a1=100, a2=100\n",
      ">33, d1=0.003, d2=0.019 g=0.001, a1=100, a2=100\n",
      ">34, d1=0.003, d2=0.018 g=0.001, a1=100, a2=100\n",
      ">35, d1=0.006, d2=0.017 g=0.001, a1=100, a2=100\n",
      ">36, d1=0.003, d2=0.016 g=0.001, a1=100, a2=100\n",
      ">37, d1=0.004, d2=0.014 g=0.001, a1=100, a2=100\n",
      ">38, d1=0.002, d2=0.013 g=0.001, a1=100, a2=100\n",
      ">39, d1=0.003, d2=0.012 g=0.000, a1=100, a2=100\n",
      ">40, d1=0.002, d2=0.011 g=0.001, a1=100, a2=100\n",
      ">41, d1=0.004, d2=0.010 g=0.001, a1=100, a2=100\n",
      ">42, d1=0.004, d2=0.009 g=0.000, a1=100, a2=100\n",
      ">43, d1=0.004, d2=0.009 g=0.001, a1=100, a2=100\n",
      ">44, d1=0.001, d2=0.009 g=0.000, a1=100, a2=100\n",
      ">45, d1=0.002, d2=0.008 g=0.001, a1=100, a2=100\n",
      ">46, d1=0.002, d2=0.007 g=0.001, a1=100, a2=100\n",
      ">47, d1=0.001, d2=0.007 g=0.000, a1=100, a2=100\n",
      ">48, d1=0.002, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">49, d1=0.002, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">50, d1=0.002, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">51, d1=0.002, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">52, d1=0.002, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">53, d1=0.004, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">54, d1=0.001, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">55, d1=0.002, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">56, d1=0.002, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">57, d1=0.002, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">58, d1=0.001, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">59, d1=0.002, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">60, d1=0.002, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">61, d1=0.001, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">62, d1=0.001, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">63, d1=0.001, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">64, d1=0.002, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">65, d1=0.002, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">66, d1=0.001, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">67, d1=0.001, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">68, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">69, d1=0.001, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">70, d1=0.001, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">71, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">72, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">73, d1=0.002, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">74, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">75, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">76, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">77, d1=0.002, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">78, d1=0.002, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">79, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">80, d1=0.002, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">81, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">82, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">83, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">84, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">85, d1=0.002, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">86, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">87, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">88, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">89, d1=0.002, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">90, d1=0.001, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">91, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">92, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">93, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">94, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">95, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">96, d1=0.001, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">97, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">98, d1=0.002, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">99, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">100, d1=0.002, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">101, d1=0.002, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">102, d1=0.002, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">103, d1=0.003, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">104, d1=0.001, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">105, d1=0.002, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">106, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">107, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">108, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">109, d1=0.001, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">110, d1=0.002, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">111, d1=0.002, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">112, d1=0.002, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">113, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">114, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">115, d1=0.001, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">116, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">117, d1=0.002, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">118, d1=0.002, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">119, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">120, d1=0.002, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">121, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">122, d1=0.002, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">123, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">124, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">125, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">126, d1=0.001, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">127, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">128, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">129, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">130, d1=0.001, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">131, d1=0.001, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">132, d1=0.001, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">133, d1=0.001, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">134, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">135, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">136, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">137, d1=0.001, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">138, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">139, d1=0.000, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">140, d1=0.001, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">141, d1=0.001, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">142, d1=0.001, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">143, d1=0.001, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">144, d1=0.001, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">145, d1=0.001, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">146, d1=0.001, d2=0.001 g=0.000, a1=100, a2=100\n",
      ">147, d1=0.001, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">148, d1=0.001, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">149, d1=0.001, d2=0.001 g=0.002, a1=100, a2=100\n",
      ">150, d1=0.001, d2=0.000 g=0.896, a1=100, a2=100\n",
      ">151, d1=0.000, d2=0.000 g=1.323, a1=100, a2=100\n",
      ">152, d1=0.000, d2=0.001 g=0.583, a1=100, a2=100\n",
      ">153, d1=0.000, d2=0.001 g=0.229, a1=100, a2=100\n",
      ">154, d1=0.001, d2=0.000 g=4.964, a1=100, a2=100\n",
      ">155, d1=0.011, d2=0.001 g=4.649, a1=100, a2=100\n",
      ">156, d1=0.009, d2=0.003 g=3.768, a1=100, a2=100\n",
      ">157, d1=0.010, d2=0.008 g=3.560, a1=100, a2=100\n",
      ">158, d1=0.006, d2=0.007 g=3.477, a1=100, a2=100\n",
      ">159, d1=0.006, d2=0.007 g=3.485, a1=100, a2=100\n",
      ">160, d1=0.006, d2=0.008 g=3.389, a1=100, a2=100\n",
      ">161, d1=0.036, d2=0.009 g=3.568, a1=98, a2=100\n",
      ">162, d1=0.004, d2=0.007 g=4.259, a1=100, a2=100\n",
      ">163, d1=0.001, d2=0.004 g=4.391, a1=100, a2=100\n",
      ">164, d1=0.002, d2=0.004 g=4.544, a1=100, a2=100\n",
      ">165, d1=0.002, d2=0.004 g=4.506, a1=100, a2=100\n",
      ">166, d1=0.001, d2=0.005 g=4.637, a1=100, a2=100\n",
      ">167, d1=0.001, d2=0.004 g=4.455, a1=100, a2=100\n",
      ">168, d1=0.002, d2=0.005 g=4.200, a1=100, a2=100\n",
      ">169, d1=0.001, d2=0.006 g=4.503, a1=100, a2=100\n",
      ">170, d1=0.002, d2=0.005 g=4.949, a1=100, a2=100\n",
      ">171, d1=0.001, d2=0.003 g=5.408, a1=100, a2=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">172, d1=0.002, d2=0.002 g=5.571, a1=100, a2=100\n",
      ">173, d1=0.001, d2=0.002 g=5.784, a1=100, a2=100\n",
      ">174, d1=0.003, d2=0.001 g=5.843, a1=100, a2=100\n",
      ">175, d1=0.002, d2=0.002 g=5.788, a1=100, a2=100\n",
      ">176, d1=0.001, d2=0.002 g=6.188, a1=100, a2=100\n",
      ">177, d1=0.001, d2=0.001 g=6.084, a1=100, a2=100\n",
      ">178, d1=0.001, d2=0.002 g=5.893, a1=100, a2=100\n",
      ">179, d1=0.002, d2=0.002 g=5.861, a1=100, a2=100\n",
      ">180, d1=0.001, d2=0.002 g=5.771, a1=100, a2=100\n",
      ">181, d1=0.001, d2=0.002 g=4.901, a1=100, a2=100\n",
      ">182, d1=0.001, d2=0.003 g=2.334, a1=100, a2=100\n",
      ">183, d1=0.001, d2=0.008 g=2.126, a1=100, a2=100\n",
      ">184, d1=0.001, d2=0.010 g=0.474, a1=100, a2=100\n",
      ">185, d1=0.002, d2=0.000 g=4.472, a1=100, a2=100\n",
      ">186, d1=0.091, d2=0.004 g=0.397, a1=98, a2=100\n",
      ">187, d1=0.016, d2=0.001 g=3.951, a1=100, a2=100\n",
      ">188, d1=0.094, d2=0.013 g=0.580, a1=98, a2=100\n",
      ">189, d1=0.004, d2=0.004 g=1.802, a1=100, a2=100\n",
      ">190, d1=0.020, d2=0.022 g=0.643, a1=100, a2=100\n",
      ">191, d1=0.026, d2=0.001 g=4.123, a1=100, a2=100\n",
      ">192, d1=0.048, d2=0.012 g=1.933, a1=100, a2=100\n",
      ">193, d1=0.139, d2=0.001 g=4.494, a1=96, a2=100\n",
      ">194, d1=0.032, d2=0.101 g=1.593, a1=100, a2=100\n",
      ">195, d1=0.248, d2=0.057 g=0.590, a1=95, a2=100\n",
      ">196, d1=0.173, d2=0.044 g=1.612, a1=96, a2=100\n",
      ">197, d1=0.205, d2=0.214 g=2.792, a1=92, a2=100\n",
      ">198, d1=0.417, d2=0.148 g=1.758, a1=82, a2=100\n",
      ">199, d1=0.135, d2=0.127 g=1.721, a1=96, a2=100\n",
      ">200, d1=0.199, d2=0.124 g=1.614, a1=95, a2=100\n",
      ">201, d1=0.344, d2=0.271 g=2.218, a1=87, a2=100\n",
      ">202, d1=0.425, d2=0.263 g=1.915, a1=82, a2=96\n",
      ">203, d1=0.434, d2=0.248 g=2.005, a1=79, a2=100\n",
      ">204, d1=0.404, d2=0.225 g=1.634, a1=84, a2=100\n",
      ">205, d1=0.409, d2=0.315 g=1.415, a1=82, a2=100\n",
      ">206, d1=0.536, d2=0.488 g=1.538, a1=70, a2=70\n",
      ">207, d1=0.496, d2=0.342 g=1.847, a1=75, a2=100\n",
      ">208, d1=0.429, d2=0.229 g=1.791, a1=78, a2=100\n",
      ">209, d1=0.302, d2=0.198 g=1.282, a1=89, a2=100\n",
      ">210, d1=0.330, d2=0.257 g=1.492, a1=89, a2=95\n",
      ">211, d1=0.490, d2=0.313 g=1.582, a1=73, a2=89\n",
      ">212, d1=0.513, d2=0.299 g=1.685, a1=76, a2=95\n",
      ">213, d1=0.584, d2=0.327 g=1.633, a1=65, a2=100\n",
      ">214, d1=0.604, d2=0.258 g=1.891, a1=71, a2=100\n",
      ">215, d1=0.535, d2=0.221 g=1.895, a1=67, a2=100\n",
      ">216, d1=0.434, d2=0.212 g=1.851, a1=85, a2=100\n",
      ">217, d1=0.401, d2=0.201 g=2.486, a1=95, a2=100\n",
      ">218, d1=0.546, d2=0.268 g=2.098, a1=71, a2=95\n",
      ">219, d1=0.395, d2=0.188 g=2.375, a1=79, a2=100\n",
      ">220, d1=0.338, d2=0.196 g=1.780, a1=90, a2=100\n",
      ">221, d1=0.302, d2=0.156 g=2.213, a1=89, a2=100\n",
      ">222, d1=0.387, d2=0.197 g=1.924, a1=82, a2=100\n",
      ">223, d1=0.389, d2=0.257 g=1.608, a1=85, a2=100\n",
      ">224, d1=0.312, d2=0.197 g=1.299, a1=90, a2=100\n",
      ">225, d1=0.217, d2=0.172 g=1.541, a1=95, a2=100\n",
      ">226, d1=0.301, d2=0.206 g=1.345, a1=93, a2=100\n",
      ">227, d1=0.364, d2=0.235 g=1.081, a1=81, a2=100\n",
      ">228, d1=0.411, d2=0.253 g=0.932, a1=84, a2=100\n",
      ">229, d1=0.447, d2=0.298 g=1.125, a1=82, a2=87\n",
      ">230, d1=0.420, d2=0.233 g=1.306, a1=84, a2=100\n",
      ">231, d1=0.434, d2=0.296 g=1.610, a1=81, a2=100\n",
      ">232, d1=0.430, d2=0.239 g=1.817, a1=85, a2=100\n",
      ">233, d1=0.382, d2=0.209 g=1.942, a1=87, a2=100\n",
      ">234, d1=0.322, d2=0.202 g=2.089, a1=93, a2=100\n",
      ">235, d1=0.318, d2=0.184 g=1.741, a1=92, a2=100\n",
      ">236, d1=0.204, d2=0.140 g=2.204, a1=98, a2=100\n",
      ">237, d1=0.342, d2=0.154 g=2.757, a1=85, a2=100\n",
      ">238, d1=0.300, d2=0.143 g=2.442, a1=90, a2=100\n",
      ">239, d1=0.250, d2=0.138 g=2.831, a1=90, a2=100\n",
      ">240, d1=0.204, d2=0.104 g=2.769, a1=95, a2=100\n",
      ">241, d1=0.111, d2=0.078 g=2.641, a1=98, a2=100\n",
      ">242, d1=0.118, d2=0.089 g=2.323, a1=98, a2=100\n",
      ">243, d1=0.154, d2=0.080 g=1.875, a1=96, a2=100\n",
      ">244, d1=0.224, d2=0.102 g=1.631, a1=93, a2=100\n",
      ">245, d1=0.295, d2=0.162 g=2.194, a1=90, a2=100\n",
      ">246, d1=0.311, d2=0.154 g=1.786, a1=95, a2=100\n",
      ">247, d1=0.210, d2=0.134 g=2.025, a1=100, a2=100\n",
      ">248, d1=0.266, d2=0.102 g=2.153, a1=96, a2=100\n",
      ">249, d1=0.329, d2=0.130 g=1.692, a1=92, a2=100\n",
      ">250, d1=0.204, d2=0.114 g=1.909, a1=96, a2=100\n",
      ">251, d1=0.290, d2=0.153 g=2.118, a1=93, a2=100\n",
      ">252, d1=0.298, d2=0.136 g=2.093, a1=90, a2=100\n",
      ">253, d1=0.308, d2=0.145 g=2.129, a1=89, a2=100\n",
      ">254, d1=0.286, d2=0.150 g=1.686, a1=92, a2=100\n",
      ">255, d1=0.218, d2=0.141 g=2.643, a1=96, a2=100\n",
      ">256, d1=0.366, d2=0.139 g=2.328, a1=84, a2=100\n",
      ">257, d1=0.293, d2=0.132 g=1.610, a1=93, a2=100\n",
      ">258, d1=0.293, d2=0.204 g=1.759, a1=90, a2=100\n",
      ">259, d1=0.433, d2=0.294 g=2.051, a1=79, a2=100\n",
      ">260, d1=0.543, d2=0.281 g=1.872, a1=70, a2=100\n",
      ">261, d1=0.388, d2=0.194 g=1.373, a1=87, a2=100\n",
      ">262, d1=0.285, d2=0.170 g=1.963, a1=95, a2=100\n",
      ">263, d1=0.362, d2=0.183 g=2.057, a1=87, a2=100\n",
      ">264, d1=0.284, d2=0.163 g=1.855, a1=90, a2=100\n",
      ">265, d1=0.220, d2=0.153 g=1.558, a1=96, a2=100\n",
      ">266, d1=0.217, d2=0.172 g=2.065, a1=95, a2=100\n",
      ">267, d1=0.325, d2=0.159 g=1.919, a1=93, a2=100\n",
      ">268, d1=0.246, d2=0.135 g=1.728, a1=95, a2=100\n",
      ">269, d1=0.255, d2=0.134 g=1.063, a1=92, a2=100\n",
      ">270, d1=0.252, d2=0.124 g=1.094, a1=93, a2=100\n",
      ">271, d1=0.332, d2=0.202 g=1.712, a1=87, a2=100\n",
      ">272, d1=0.346, d2=0.162 g=1.509, a1=90, a2=100\n",
      ">273, d1=0.254, d2=0.155 g=2.571, a1=95, a2=100\n",
      ">274, d1=0.305, d2=0.159 g=1.846, a1=90, a2=100\n",
      ">275, d1=0.281, d2=0.125 g=2.299, a1=90, a2=100\n",
      ">276, d1=0.369, d2=0.153 g=1.924, a1=87, a2=100\n",
      ">277, d1=0.364, d2=0.186 g=1.207, a1=87, a2=100\n",
      ">278, d1=0.351, d2=0.167 g=1.201, a1=89, a2=100\n",
      ">279, d1=0.353, d2=0.214 g=1.364, a1=87, a2=100\n",
      ">280, d1=0.420, d2=0.297 g=2.318, a1=81, a2=100\n",
      ">281, d1=0.569, d2=0.291 g=1.799, a1=71, a2=100\n",
      ">282, d1=0.364, d2=0.196 g=2.292, a1=90, a2=100\n",
      ">283, d1=0.320, d2=0.190 g=1.888, a1=93, a2=100\n",
      ">284, d1=0.255, d2=0.161 g=1.806, a1=93, a2=100\n",
      ">285, d1=0.264, d2=0.153 g=2.339, a1=93, a2=100\n",
      ">286, d1=0.345, d2=0.182 g=2.145, a1=89, a2=100\n",
      ">287, d1=0.317, d2=0.167 g=2.018, a1=90, a2=100\n",
      ">288, d1=0.338, d2=0.160 g=2.021, a1=90, a2=100\n",
      ">289, d1=0.320, d2=0.163 g=1.490, a1=92, a2=100\n",
      ">290, d1=0.293, d2=0.138 g=1.917, a1=92, a2=100\n",
      ">291, d1=0.315, d2=0.147 g=1.990, a1=89, a2=100\n",
      ">292, d1=0.301, d2=0.153 g=2.149, a1=89, a2=100\n",
      ">293, d1=0.365, d2=0.164 g=2.243, a1=81, a2=100\n",
      ">294, d1=0.378, d2=0.183 g=1.960, a1=82, a2=100\n",
      ">295, d1=0.374, d2=0.160 g=1.466, a1=82, a2=100\n",
      ">296, d1=0.362, d2=0.184 g=1.588, a1=73, a2=100\n",
      ">297, d1=0.346, d2=0.193 g=1.507, a1=85, a2=100\n",
      ">298, d1=0.281, d2=0.204 g=1.246, a1=92, a2=100\n",
      ">299, d1=0.307, d2=0.189 g=1.664, a1=90, a2=100\n",
      ">300, d1=0.350, d2=0.173 g=1.717, a1=89, a2=100\n",
      ">301, d1=0.256, d2=0.172 g=1.519, a1=89, a2=100\n",
      ">302, d1=0.188, d2=0.172 g=1.124, a1=95, a2=100\n",
      ">303, d1=0.276, d2=0.238 g=0.984, a1=90, a2=100\n",
      ">304, d1=0.335, d2=0.289 g=1.788, a1=84, a2=92\n",
      ">305, d1=0.466, d2=0.258 g=1.099, a1=81, a2=100\n",
      ">306, d1=0.402, d2=0.300 g=1.774, a1=82, a2=100\n",
      ">307, d1=0.691, d2=0.320 g=1.619, a1=60, a2=100\n",
      ">308, d1=0.554, d2=0.266 g=1.145, a1=71, a2=100\n",
      ">309, d1=0.370, d2=0.215 g=0.696, a1=82, a2=100\n",
      ">310, d1=0.366, d2=0.265 g=1.068, a1=82, a2=100\n",
      ">311, d1=0.576, d2=0.357 g=1.396, a1=65, a2=100\n",
      ">312, d1=0.702, d2=0.345 g=1.491, a1=70, a2=100\n",
      ">313, d1=0.618, d2=0.286 g=1.422, a1=70, a2=100\n",
      ">314, d1=0.666, d2=0.386 g=1.354, a1=62, a2=100\n",
      ">315, d1=0.606, d2=0.274 g=1.780, a1=65, a2=100\n",
      ">316, d1=0.565, d2=0.213 g=2.406, a1=68, a2=100\n",
      ">317, d1=0.566, d2=0.243 g=1.883, a1=68, a2=100\n",
      ">318, d1=0.340, d2=0.188 g=1.290, a1=85, a2=100\n",
      ">319, d1=0.336, d2=0.278 g=1.494, a1=87, a2=100\n",
      ">320, d1=0.391, d2=0.252 g=1.153, a1=85, a2=100\n",
      ">321, d1=0.362, d2=0.302 g=1.332, a1=84, a2=100\n",
      ">322, d1=0.457, d2=0.330 g=1.580, a1=76, a2=100\n",
      ">323, d1=0.472, d2=0.329 g=1.298, a1=71, a2=100\n",
      ">324, d1=0.442, d2=0.309 g=1.310, a1=81, a2=100\n",
      ">325, d1=0.526, d2=0.316 g=0.703, a1=76, a2=100\n",
      ">326, d1=0.474, d2=0.385 g=1.282, a1=82, a2=85\n",
      ">327, d1=0.651, d2=0.424 g=0.939, a1=65, a2=81\n",
      ">328, d1=0.416, d2=0.293 g=1.214, a1=82, a2=92\n",
      ">329, d1=0.491, d2=0.354 g=1.889, a1=78, a2=79\n",
      ">330, d1=0.652, d2=0.375 g=1.055, a1=70, a2=82\n",
      ">331, d1=0.509, d2=0.274 g=0.954, a1=76, a2=100\n",
      ">332, d1=0.478, d2=0.295 g=1.487, a1=81, a2=100\n",
      ">333, d1=0.659, d2=0.319 g=1.254, a1=59, a2=100\n",
      ">334, d1=0.565, d2=0.412 g=1.379, a1=73, a2=87\n",
      ">335, d1=0.633, d2=0.358 g=1.513, a1=65, a2=100\n",
      ">336, d1=0.633, d2=0.431 g=1.356, a1=57, a2=100\n",
      ">337, d1=0.722, d2=0.458 g=1.301, a1=54, a2=100\n",
      ">338, d1=0.547, d2=0.290 g=1.158, a1=73, a2=100\n",
      ">339, d1=0.550, d2=0.397 g=1.436, a1=70, a2=100\n",
      ">340, d1=0.619, d2=0.331 g=1.105, a1=67, a2=100\n",
      ">341, d1=0.526, d2=0.329 g=1.205, a1=73, a2=100\n",
      ">342, d1=0.631, d2=0.382 g=1.328, a1=62, a2=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">343, d1=0.716, d2=0.449 g=1.312, a1=57, a2=96\n",
      ">344, d1=0.599, d2=0.354 g=1.312, a1=67, a2=100\n",
      ">345, d1=0.580, d2=0.440 g=1.418, a1=65, a2=100\n",
      ">346, d1=0.667, d2=0.443 g=1.377, a1=62, a2=100\n",
      ">347, d1=0.625, d2=0.414 g=1.035, a1=68, a2=96\n",
      ">348, d1=0.563, d2=0.522 g=0.954, a1=67, a2=65\n",
      ">349, d1=0.667, d2=0.577 g=1.329, a1=60, a2=90\n",
      ">350, d1=0.737, d2=0.498 g=0.881, a1=56, a2=100\n",
      ">351, d1=0.776, d2=0.684 g=1.207, a1=53, a2=62\n",
      ">352, d1=0.881, d2=0.458 g=1.092, a1=39, a2=89\n",
      ">353, d1=0.723, d2=0.575 g=1.104, a1=59, a2=60\n",
      ">354, d1=0.975, d2=0.599 g=1.078, a1=40, a2=85\n",
      ">355, d1=0.881, d2=0.480 g=1.009, a1=40, a2=79\n",
      ">356, d1=0.566, d2=0.436 g=1.282, a1=71, a2=100\n",
      ">357, d1=0.805, d2=0.561 g=1.088, a1=46, a2=82\n",
      ">358, d1=0.834, d2=0.597 g=1.212, a1=51, a2=68\n",
      ">359, d1=0.850, d2=0.530 g=1.044, a1=48, a2=100\n",
      ">360, d1=0.906, d2=0.718 g=0.920, a1=37, a2=29\n",
      ">361, d1=0.893, d2=0.488 g=0.866, a1=39, a2=100\n",
      ">362, d1=0.899, d2=0.675 g=0.783, a1=45, a2=54\n",
      ">363, d1=0.754, d2=0.602 g=0.815, a1=50, a2=81\n",
      ">364, d1=0.895, d2=0.685 g=0.805, a1=45, a2=42\n",
      ">365, d1=0.904, d2=0.720 g=1.035, a1=39, a2=31\n",
      ">366, d1=0.853, d2=0.548 g=0.962, a1=51, a2=87\n",
      ">367, d1=0.711, d2=0.509 g=0.981, a1=54, a2=98\n",
      ">368, d1=0.685, d2=0.537 g=1.087, a1=59, a2=87\n",
      ">369, d1=0.754, d2=0.543 g=1.048, a1=46, a2=84\n",
      ">370, d1=0.671, d2=0.552 g=1.041, a1=56, a2=75\n",
      ">371, d1=0.733, d2=0.498 g=0.976, a1=46, a2=100\n",
      ">372, d1=0.654, d2=0.477 g=0.875, a1=60, a2=90\n",
      ">373, d1=0.649, d2=0.605 g=0.810, a1=57, a2=67\n",
      ">374, d1=0.765, d2=0.524 g=0.774, a1=48, a2=95\n",
      ">375, d1=0.662, d2=0.649 g=0.929, a1=60, a2=76\n",
      ">376, d1=0.844, d2=0.623 g=1.008, a1=37, a2=64\n",
      ">377, d1=0.783, d2=0.553 g=1.080, a1=48, a2=85\n",
      ">378, d1=0.718, d2=0.553 g=1.236, a1=54, a2=100\n",
      ">379, d1=0.676, d2=0.441 g=1.141, a1=59, a2=100\n",
      ">380, d1=0.531, d2=0.542 g=1.150, a1=73, a2=78\n",
      ">381, d1=0.715, d2=0.553 g=1.025, a1=56, a2=100\n",
      ">382, d1=0.819, d2=0.572 g=1.064, a1=51, a2=85\n",
      ">383, d1=0.801, d2=0.566 g=1.005, a1=42, a2=68\n",
      ">384, d1=0.731, d2=0.597 g=1.146, a1=54, a2=79\n",
      ">385, d1=0.820, d2=0.498 g=1.260, a1=42, a2=84\n",
      ">386, d1=0.802, d2=0.527 g=1.212, a1=45, a2=82\n",
      ">387, d1=0.653, d2=0.452 g=1.283, a1=59, a2=87\n",
      ">388, d1=0.630, d2=0.420 g=1.169, a1=65, a2=100\n",
      ">389, d1=0.602, d2=0.494 g=0.925, a1=64, a2=95\n",
      ">390, d1=0.608, d2=0.515 g=0.899, a1=64, a2=93\n",
      ">391, d1=0.562, d2=0.483 g=1.030, a1=73, a2=95\n",
      ">392, d1=0.525, d2=0.413 g=1.011, a1=78, a2=92\n",
      ">393, d1=0.574, d2=0.494 g=0.883, a1=70, a2=79\n",
      ">394, d1=0.582, d2=0.495 g=0.785, a1=70, a2=89\n",
      ">395, d1=0.503, d2=0.472 g=0.768, a1=78, a2=70\n",
      ">396, d1=0.503, d2=0.503 g=0.913, a1=79, a2=81\n",
      ">397, d1=0.652, d2=0.593 g=1.166, a1=67, a2=76\n",
      ">398, d1=0.790, d2=0.512 g=0.990, a1=43, a2=100\n",
      ">399, d1=0.693, d2=0.655 g=1.001, a1=62, a2=53\n",
      ">400, d1=0.806, d2=0.641 g=1.103, a1=50, a2=48\n",
      ">401, d1=0.804, d2=0.527 g=0.909, a1=42, a2=78\n",
      ">402, d1=0.664, d2=0.596 g=0.957, a1=60, a2=46\n",
      ">403, d1=0.756, d2=0.623 g=0.935, a1=54, a2=59\n",
      ">404, d1=0.719, d2=0.595 g=1.167, a1=56, a2=71\n",
      ">405, d1=0.800, d2=0.520 g=1.089, a1=42, a2=100\n",
      ">406, d1=0.761, d2=0.580 g=0.962, a1=42, a2=85\n",
      ">407, d1=0.686, d2=0.538 g=1.019, a1=56, a2=95\n",
      ">408, d1=0.656, d2=0.539 g=1.163, a1=62, a2=92\n",
      ">409, d1=0.680, d2=0.510 g=1.183, a1=53, a2=100\n",
      ">410, d1=0.602, d2=0.437 g=1.113, a1=68, a2=100\n",
      ">411, d1=0.572, d2=0.485 g=1.222, a1=70, a2=100\n",
      ">412, d1=0.560, d2=0.369 g=1.421, a1=65, a2=100\n",
      ">413, d1=0.562, d2=0.438 g=1.263, a1=65, a2=100\n",
      ">414, d1=0.623, d2=0.487 g=1.146, a1=67, a2=85\n",
      ">415, d1=0.551, d2=0.470 g=1.279, a1=70, a2=100\n",
      ">416, d1=0.626, d2=0.473 g=1.125, a1=62, a2=100\n",
      ">417, d1=0.608, d2=0.513 g=1.095, a1=60, a2=100\n",
      ">418, d1=0.630, d2=0.503 g=1.114, a1=60, a2=100\n",
      ">419, d1=0.670, d2=0.491 g=1.155, a1=62, a2=100\n",
      ">420, d1=0.645, d2=0.474 g=1.019, a1=59, a2=87\n",
      ">421, d1=0.561, d2=0.484 g=1.167, a1=73, a2=79\n",
      ">422, d1=0.665, d2=0.471 g=1.063, a1=59, a2=100\n",
      ">423, d1=0.532, d2=0.426 g=1.210, a1=76, a2=100\n",
      ">424, d1=0.557, d2=0.427 g=1.008, a1=78, a2=100\n",
      ">425, d1=0.574, d2=0.500 g=1.041, a1=70, a2=100\n",
      ">426, d1=0.655, d2=0.479 g=1.153, a1=62, a2=100\n",
      ">427, d1=0.718, d2=0.478 g=1.072, a1=62, a2=100\n",
      ">428, d1=0.628, d2=0.494 g=1.156, a1=78, a2=87\n",
      ">429, d1=0.687, d2=0.488 g=1.271, a1=64, a2=68\n",
      ">430, d1=0.752, d2=0.533 g=1.029, a1=56, a2=68\n",
      ">431, d1=0.842, d2=0.581 g=0.995, a1=46, a2=67\n",
      ">432, d1=0.716, d2=0.513 g=0.887, a1=57, a2=81\n",
      ">433, d1=0.694, d2=0.517 g=0.856, a1=59, a2=84\n",
      ">434, d1=0.880, d2=0.595 g=0.895, a1=46, a2=71\n",
      ">435, d1=0.741, d2=0.484 g=1.051, a1=56, a2=100\n",
      ">436, d1=0.751, d2=0.449 g=0.875, a1=56, a2=100\n",
      ">437, d1=0.525, d2=0.406 g=0.938, a1=75, a2=100\n",
      ">438, d1=0.456, d2=0.404 g=0.941, a1=79, a2=100\n",
      ">439, d1=0.610, d2=0.568 g=0.866, a1=60, a2=84\n",
      ">440, d1=0.674, d2=0.532 g=0.945, a1=59, a2=78\n",
      ">441, d1=0.649, d2=0.502 g=1.000, a1=64, a2=85\n",
      ">442, d1=0.571, d2=0.416 g=1.308, a1=71, a2=93\n",
      ">443, d1=0.526, d2=0.343 g=1.136, a1=71, a2=100\n",
      ">444, d1=0.489, d2=0.356 g=0.983, a1=78, a2=100\n",
      ">445, d1=0.469, d2=0.400 g=1.003, a1=81, a2=100\n",
      ">446, d1=0.464, d2=0.336 g=0.865, a1=82, a2=100\n",
      ">447, d1=0.535, d2=0.441 g=0.962, a1=76, a2=100\n",
      ">448, d1=0.544, d2=0.379 g=1.113, a1=76, a2=100\n",
      ">449, d1=0.556, d2=0.405 g=0.990, a1=79, a2=100\n",
      ">450, d1=0.534, d2=0.422 g=1.039, a1=75, a2=100\n"
     ]
    }
   ],
   "source": [
    "# example of training an unstable gan for generating a handwritten digit\n",
    "from os import makedirs\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(28,28,1)):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    # downsample to 14x14\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init,\n",
    "    input_shape=in_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample to 7x7\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    model.add(Dense(n_nodes, kernel_initializer=init, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    # upsample to 14x14\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same',\n",
    "    kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 28x28\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same',\n",
    "    kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # output 28x28x1\n",
    "    model.add(Conv2D(1, (7,7), activation='tanh', padding='same', kernel_initializer=init))\n",
    "    return model\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "# load mnist images\n",
    "def load_real_samples():\n",
    "    # load dataset\n",
    "    (trainX, trainy), (_, _) = load_data()\n",
    "    # expand to 3d, e.g. add channels\n",
    "    X = expand_dims(trainX, axis=-1)\n",
    "    # select all of the examples for a given class\n",
    "    selected_ix = trainy == 8\n",
    "    X = X[selected_ix]\n",
    "    # convert from ints to floats\n",
    "    X = X.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    return X\n",
    "\n",
    "# # select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # select images\n",
    "    X = dataset[ix]\n",
    "    # generate class labels\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, latent_dim, n_samples=100):\n",
    "    # prepare fake examples\n",
    "    X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    X = (X + 1) / 2.0\n",
    "    # plot images\n",
    "    for i in range(10 * 10):\n",
    "        # define subplot\n",
    "        pyplot.subplot(10, 10, 1 + i)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
    "    # save plot to file\n",
    "    pyplot.savefig('results_collapse/generated_plot_%03d.png' % (step+1))\n",
    "    pyplot.close()\n",
    "    # save the generator model\n",
    "    g_model.save('results_collapse/model_%03d.h5' % (step+1))\n",
    "    \n",
    "# create a line plot of loss for the gan and save to file\n",
    "def plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist):\n",
    "    # plot loss\n",
    "    pyplot.subplot(2, 1, 1)\n",
    "    pyplot.plot(d1_hist, label='d-real')\n",
    "    pyplot.plot(d2_hist, label='d-fake')\n",
    "    pyplot.plot(g_hist, label='gen')\n",
    "    pyplot.legend()\n",
    "    # plot discriminator accuracy\n",
    "    pyplot.subplot(2, 1, 2)\n",
    "    pyplot.plot(a1_hist, label='acc-real')\n",
    "    pyplot.plot(a2_hist, label='acc-fake')\n",
    "    pyplot.legend()\n",
    "    # save plot to file\n",
    "    pyplot.savefig('results_collapse/plot_line_plot_loss.png')\n",
    "    pyplot.close()\n",
    "    \n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=10, n_batch=128):\n",
    "    # calculate the number of batches per epoch\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    # calculate the total iterations based on batch and epoch\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    # calculate the number of samples in half a batch\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # prepare lists for storing stats each iteration\n",
    "    d1_hist, d2_hist, g_hist, a1_hist, a2_hist = list(), list(), list(), list(), list()\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        # get randomly selected ✬real✬ samples\n",
    "        X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "        \n",
    "        # update discriminator model weights\n",
    "        for j in range(2):\n",
    "            d_loss1, d_acc1 = d_model.train_on_batch(X_real, y_real)\n",
    "        \n",
    "        # generate ✬fake✬ examples\n",
    "        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        \n",
    "        # update discriminator model weights\n",
    "        for j in range(2):\n",
    "            d_loss2, d_acc2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "        \n",
    "        # prepare points in latent space as input for the generator\n",
    "        X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = ones((n_batch, 1))\n",
    "        \n",
    "        # update the generator via the discriminator✬s error\n",
    "        for j in range(3): # Generator updated twice per each discriminator update (to avoid convergence failure)\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "        \n",
    "        # summarize loss on this batch\n",
    "        print('>%d, d1=%.3f, d2=%.3f g=%.3f, a1=%d, a2=%d' %\n",
    "                    (i+1, d_loss1, d_loss2, g_loss, int(100*d_acc1), int(100*d_acc2)))\n",
    "        # record history\n",
    "        d1_hist.append(d_loss1)\n",
    "        d2_hist.append(d_loss2)\n",
    "        g_hist.append(g_loss)\n",
    "        a1_hist.append(d_acc1)\n",
    "        a2_hist.append(d_acc2)\n",
    "        # evaluate the model performance every ✬epoch✬\n",
    "        if (i+1) % bat_per_epo == 0:\n",
    "            summarize_performance(i, g_model, latent_dim)\n",
    "    plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist)\n",
    "\n",
    "# make folder for results\n",
    "makedirs('results_collapse', exist_ok=True)\n",
    "# size of the latent space\n",
    "latent_dim = 1\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# load image data\n",
    "dataset = load_real_samples()\n",
    "print(dataset.shape)\n",
    "# train model\n",
    "train(generator, discriminator, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
