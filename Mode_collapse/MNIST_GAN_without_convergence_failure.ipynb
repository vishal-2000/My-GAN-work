{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5851, 28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, d1=0.820, d2=0.708 g=0.056, a1=48, a2=18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2, d1=0.138, d2=0.926 g=0.010, a1=100, a2=0\n",
      ">3, d1=0.060, d2=0.930 g=0.003, a1=100, a2=0\n",
      ">4, d1=0.044, d2=0.968 g=0.002, a1=100, a2=0\n",
      ">5, d1=0.029, d2=0.894 g=0.002, a1=100, a2=0\n",
      ">6, d1=0.029, d2=0.881 g=0.003, a1=100, a2=0\n",
      ">7, d1=0.021, d2=0.846 g=0.003, a1=100, a2=0\n",
      ">8, d1=0.023, d2=0.781 g=0.004, a1=100, a2=0\n",
      ">9, d1=0.020, d2=0.745 g=0.003, a1=100, a2=17\n",
      ">10, d1=0.016, d2=0.738 g=0.003, a1=100, a2=23\n",
      ">11, d1=0.015, d2=0.718 g=0.003, a1=100, a2=35\n",
      ">12, d1=0.016, d2=0.696 g=0.002, a1=100, a2=56\n",
      ">13, d1=0.019, d2=0.694 g=0.003, a1=100, a2=51\n",
      ">14, d1=0.017, d2=0.671 g=0.002, a1=100, a2=87\n",
      ">15, d1=0.012, d2=0.657 g=0.002, a1=100, a2=98\n",
      ">16, d1=0.009, d2=0.645 g=0.002, a1=100, a2=100\n",
      ">17, d1=0.009, d2=0.626 g=0.003, a1=100, a2=100\n",
      ">18, d1=0.009, d2=0.620 g=0.002, a1=100, a2=100\n",
      ">19, d1=0.007, d2=0.605 g=0.002, a1=100, a2=100\n",
      ">20, d1=0.008, d2=0.585 g=0.001, a1=100, a2=100\n",
      ">21, d1=0.016, d2=0.565 g=0.002, a1=100, a2=100\n",
      ">22, d1=0.012, d2=0.538 g=0.002, a1=100, a2=100\n",
      ">23, d1=0.012, d2=0.520 g=0.002, a1=100, a2=100\n",
      ">24, d1=0.012, d2=0.490 g=0.002, a1=100, a2=100\n",
      ">25, d1=0.020, d2=0.463 g=0.002, a1=100, a2=100\n",
      ">26, d1=0.019, d2=0.439 g=0.002, a1=100, a2=100\n",
      ">27, d1=0.016, d2=0.416 g=0.001, a1=100, a2=100\n",
      ">28, d1=0.012, d2=0.372 g=0.002, a1=100, a2=100\n",
      ">29, d1=0.013, d2=0.350 g=0.002, a1=100, a2=100\n",
      ">30, d1=0.012, d2=0.329 g=0.001, a1=100, a2=100\n",
      ">31, d1=0.024, d2=0.306 g=0.001, a1=100, a2=100\n",
      ">32, d1=0.018, d2=0.281 g=0.001, a1=100, a2=100\n",
      ">33, d1=0.011, d2=0.250 g=0.002, a1=100, a2=100\n",
      ">34, d1=0.013, d2=0.235 g=0.001, a1=100, a2=100\n",
      ">35, d1=0.013, d2=0.218 g=0.002, a1=100, a2=100\n",
      ">36, d1=0.016, d2=0.194 g=0.001, a1=100, a2=100\n",
      ">37, d1=0.026, d2=0.187 g=0.001, a1=100, a2=100\n",
      ">38, d1=0.012, d2=0.165 g=0.001, a1=100, a2=100\n",
      ">39, d1=0.010, d2=0.151 g=0.001, a1=100, a2=100\n",
      ">40, d1=0.011, d2=0.134 g=0.001, a1=100, a2=100\n",
      ">41, d1=0.013, d2=0.123 g=0.001, a1=100, a2=100\n",
      ">42, d1=0.013, d2=0.116 g=0.001, a1=100, a2=100\n",
      ">43, d1=0.017, d2=0.106 g=0.001, a1=100, a2=100\n",
      ">44, d1=0.009, d2=0.095 g=0.001, a1=100, a2=100\n",
      ">45, d1=0.008, d2=0.088 g=0.001, a1=100, a2=100\n",
      ">46, d1=0.015, d2=0.079 g=0.001, a1=100, a2=100\n",
      ">47, d1=0.010, d2=0.073 g=0.001, a1=100, a2=100\n",
      ">48, d1=0.010, d2=0.068 g=0.001, a1=100, a2=100\n",
      ">49, d1=0.008, d2=0.064 g=0.001, a1=100, a2=100\n",
      ">50, d1=0.017, d2=0.058 g=0.001, a1=100, a2=100\n",
      ">51, d1=0.010, d2=0.057 g=0.001, a1=100, a2=100\n",
      ">52, d1=0.008, d2=0.051 g=0.001, a1=100, a2=100\n",
      ">53, d1=0.006, d2=0.047 g=0.001, a1=100, a2=100\n",
      ">54, d1=0.007, d2=0.043 g=0.000, a1=100, a2=100\n",
      ">55, d1=0.006, d2=0.042 g=0.000, a1=100, a2=100\n",
      ">56, d1=0.004, d2=0.037 g=0.001, a1=100, a2=100\n",
      ">57, d1=0.010, d2=0.037 g=0.000, a1=100, a2=100\n",
      ">58, d1=0.008, d2=0.034 g=0.000, a1=100, a2=100\n",
      ">59, d1=0.006, d2=0.033 g=0.001, a1=100, a2=100\n",
      ">60, d1=0.006, d2=0.030 g=0.001, a1=100, a2=100\n",
      ">61, d1=0.008, d2=0.028 g=0.000, a1=100, a2=100\n",
      ">62, d1=0.004, d2=0.027 g=0.000, a1=100, a2=100\n",
      ">63, d1=0.004, d2=0.026 g=0.000, a1=100, a2=100\n",
      ">64, d1=0.013, d2=0.025 g=0.000, a1=100, a2=100\n",
      ">65, d1=0.005, d2=0.023 g=0.000, a1=100, a2=100\n",
      ">66, d1=0.006, d2=0.023 g=0.000, a1=100, a2=100\n",
      ">67, d1=0.006, d2=0.021 g=0.000, a1=100, a2=100\n",
      ">68, d1=0.004, d2=0.019 g=0.000, a1=100, a2=100\n",
      ">69, d1=0.005, d2=0.019 g=0.000, a1=100, a2=100\n",
      ">70, d1=0.004, d2=0.018 g=0.000, a1=100, a2=100\n",
      ">71, d1=0.004, d2=0.017 g=0.000, a1=100, a2=100\n",
      ">72, d1=0.005, d2=0.015 g=0.000, a1=100, a2=100\n",
      ">73, d1=0.005, d2=0.017 g=0.000, a1=100, a2=100\n",
      ">74, d1=0.006, d2=0.014 g=0.000, a1=100, a2=100\n",
      ">75, d1=0.003, d2=0.014 g=0.000, a1=100, a2=100\n",
      ">76, d1=0.005, d2=0.015 g=0.000, a1=100, a2=100\n",
      ">77, d1=0.002, d2=0.012 g=0.000, a1=100, a2=100\n",
      ">78, d1=0.003, d2=0.013 g=0.000, a1=100, a2=100\n",
      ">79, d1=0.003, d2=0.011 g=0.000, a1=100, a2=100\n",
      ">80, d1=0.004, d2=0.012 g=0.000, a1=100, a2=100\n",
      ">81, d1=0.004, d2=0.011 g=0.000, a1=100, a2=100\n",
      ">82, d1=0.004, d2=0.010 g=0.000, a1=100, a2=100\n",
      ">83, d1=0.003, d2=0.011 g=0.000, a1=100, a2=100\n",
      ">84, d1=0.004, d2=0.010 g=0.000, a1=100, a2=100\n",
      ">85, d1=0.003, d2=0.009 g=0.000, a1=100, a2=100\n",
      ">86, d1=0.003, d2=0.010 g=0.000, a1=100, a2=100\n",
      ">87, d1=0.003, d2=0.010 g=0.000, a1=100, a2=100\n",
      ">88, d1=0.003, d2=0.008 g=0.000, a1=100, a2=100\n",
      ">89, d1=0.008, d2=0.009 g=0.000, a1=100, a2=100\n",
      ">90, d1=0.004, d2=0.010 g=0.000, a1=100, a2=100\n",
      ">91, d1=0.003, d2=0.009 g=0.000, a1=100, a2=100\n",
      ">92, d1=0.004, d2=0.008 g=0.000, a1=100, a2=100\n",
      ">93, d1=0.001, d2=0.007 g=0.000, a1=100, a2=100\n",
      ">94, d1=0.003, d2=0.007 g=0.000, a1=100, a2=100\n",
      ">95, d1=0.004, d2=0.008 g=0.000, a1=100, a2=100\n",
      ">96, d1=0.004, d2=0.008 g=0.000, a1=100, a2=100\n",
      ">97, d1=0.002, d2=0.009 g=0.000, a1=100, a2=100\n",
      ">98, d1=0.004, d2=0.007 g=0.000, a1=100, a2=100\n",
      ">99, d1=0.002, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">100, d1=0.002, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">101, d1=0.003, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">102, d1=0.004, d2=0.007 g=0.000, a1=100, a2=100\n",
      ">103, d1=0.002, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">104, d1=0.003, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">105, d1=0.003, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">106, d1=0.003, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">107, d1=0.003, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">108, d1=0.003, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">109, d1=0.002, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">110, d1=0.005, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">111, d1=0.003, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">112, d1=0.002, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">113, d1=0.002, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">114, d1=0.004, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">115, d1=0.002, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">116, d1=0.002, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">117, d1=0.004, d2=0.008 g=0.000, a1=100, a2=100\n",
      ">118, d1=0.002, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">119, d1=0.003, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">120, d1=0.004, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">121, d1=0.002, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">122, d1=0.002, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">123, d1=0.003, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">124, d1=0.002, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">125, d1=0.002, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">126, d1=0.003, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">127, d1=0.003, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">128, d1=0.003, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">129, d1=0.004, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">130, d1=0.002, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">131, d1=0.003, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">132, d1=0.003, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">133, d1=0.001, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">134, d1=0.003, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">135, d1=0.002, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">136, d1=0.002, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">137, d1=0.003, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">138, d1=0.002, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">139, d1=0.003, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">140, d1=0.003, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">141, d1=0.004, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">142, d1=0.003, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">143, d1=0.003, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">144, d1=0.005, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">145, d1=0.002, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">146, d1=0.005, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">147, d1=0.002, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">148, d1=0.002, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">149, d1=0.003, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">150, d1=0.003, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">151, d1=0.002, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">152, d1=0.002, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">153, d1=0.003, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">154, d1=0.003, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">155, d1=0.002, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">156, d1=0.003, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">157, d1=0.002, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">158, d1=0.004, d2=0.005 g=0.000, a1=100, a2=100\n",
      ">159, d1=0.004, d2=0.006 g=0.000, a1=100, a2=100\n",
      ">160, d1=0.002, d2=0.003 g=0.000, a1=100, a2=100\n",
      ">161, d1=0.003, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">162, d1=0.003, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">163, d1=0.003, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">164, d1=0.003, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">165, d1=0.004, d2=0.008 g=0.000, a1=100, a2=100\n",
      ">166, d1=0.002, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">167, d1=0.002, d2=0.004 g=0.000, a1=100, a2=100\n",
      ">168, d1=0.004, d2=0.005 g=0.001, a1=100, a2=100\n",
      ">169, d1=0.003, d2=0.006 g=0.001, a1=100, a2=100\n",
      ">170, d1=0.004, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">171, d1=0.003, d2=0.004 g=0.001, a1=100, a2=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">172, d1=0.002, d2=0.005 g=0.001, a1=100, a2=100\n",
      ">173, d1=0.002, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">174, d1=0.003, d2=0.004 g=0.001, a1=100, a2=100\n",
      ">175, d1=0.004, d2=0.007 g=0.001, a1=100, a2=100\n",
      ">176, d1=0.003, d2=0.004 g=0.001, a1=100, a2=100\n",
      ">177, d1=0.002, d2=0.005 g=0.001, a1=100, a2=100\n",
      ">178, d1=0.002, d2=0.006 g=0.001, a1=100, a2=100\n",
      ">179, d1=0.004, d2=0.006 g=0.001, a1=100, a2=100\n",
      ">180, d1=0.003, d2=0.006 g=0.001, a1=100, a2=100\n",
      ">181, d1=0.004, d2=0.006 g=0.001, a1=100, a2=100\n",
      ">182, d1=0.004, d2=0.008 g=0.001, a1=100, a2=100\n",
      ">183, d1=0.003, d2=0.006 g=0.002, a1=100, a2=100\n",
      ">184, d1=0.006, d2=0.014 g=0.002, a1=100, a2=100\n",
      ">185, d1=0.004, d2=0.004 g=0.001, a1=100, a2=100\n",
      ">186, d1=0.005, d2=0.013 g=0.001, a1=100, a2=100\n",
      ">187, d1=0.008, d2=0.006 g=0.001, a1=100, a2=100\n",
      ">188, d1=0.004, d2=0.006 g=0.001, a1=100, a2=100\n",
      ">189, d1=0.003, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">190, d1=0.002, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">191, d1=0.002, d2=0.009 g=0.001, a1=100, a2=100\n",
      ">192, d1=0.007, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">193, d1=0.002, d2=0.010 g=0.002, a1=100, a2=100\n",
      ">194, d1=0.002, d2=0.005 g=0.002, a1=100, a2=100\n",
      ">195, d1=0.008, d2=0.010 g=0.001, a1=100, a2=100\n",
      ">196, d1=0.003, d2=0.011 g=0.003, a1=100, a2=100\n",
      ">197, d1=0.008, d2=0.004 g=0.001, a1=100, a2=100\n",
      ">198, d1=0.002, d2=0.010 g=0.002, a1=100, a2=100\n",
      ">199, d1=0.005, d2=0.008 g=0.002, a1=100, a2=100\n",
      ">200, d1=0.003, d2=0.009 g=0.002, a1=100, a2=100\n",
      ">201, d1=0.004, d2=0.016 g=0.009, a1=100, a2=100\n",
      ">202, d1=0.011, d2=0.032 g=4.441, a1=100, a2=100\n",
      ">203, d1=0.020, d2=0.275 g=3.094, a1=100, a2=87\n",
      ">204, d1=0.353, d2=1.186 g=1.062, a1=87, a2=50\n",
      ">205, d1=0.000, d2=2.648 g=5.768, a1=100, a2=0\n",
      ">206, d1=0.011, d2=0.001 g=6.855, a1=100, a2=100\n",
      ">207, d1=0.040, d2=0.002 g=4.833, a1=98, a2=100\n",
      ">208, d1=0.014, d2=0.071 g=3.613, a1=100, a2=100\n",
      ">209, d1=0.019, d2=0.021 g=2.414, a1=100, a2=100\n",
      ">210, d1=0.017, d2=0.157 g=3.662, a1=100, a2=100\n",
      ">211, d1=0.028, d2=0.003 g=3.156, a1=100, a2=100\n",
      ">212, d1=0.027, d2=0.013 g=0.721, a1=100, a2=100\n",
      ">213, d1=0.017, d2=0.234 g=1.587, a1=100, a2=100\n",
      ">214, d1=0.063, d2=0.122 g=0.685, a1=98, a2=100\n",
      ">215, d1=0.101, d2=0.725 g=0.916, a1=98, a2=60\n",
      ">216, d1=1.425, d2=3.395 g=0.098, a1=10, a2=0\n",
      ">217, d1=0.101, d2=0.025 g=1.750, a1=98, a2=100\n",
      ">218, d1=0.656, d2=0.213 g=1.380, a1=60, a2=87\n",
      ">219, d1=0.065, d2=0.023 g=1.904, a1=100, a2=100\n",
      ">220, d1=0.041, d2=0.059 g=0.629, a1=100, a2=100\n",
      ">221, d1=0.060, d2=0.569 g=1.438, a1=98, a2=67\n",
      ">222, d1=0.691, d2=0.626 g=1.360, a1=62, a2=67\n",
      ">223, d1=1.045, d2=1.425 g=2.247, a1=39, a2=0\n",
      ">224, d1=1.022, d2=0.202 g=1.322, a1=45, a2=100\n",
      ">225, d1=0.539, d2=0.631 g=1.858, a1=78, a2=70\n",
      ">226, d1=0.515, d2=0.209 g=1.421, a1=70, a2=100\n",
      ">227, d1=0.492, d2=0.696 g=1.727, a1=75, a2=57\n",
      ">228, d1=0.668, d2=0.380 g=1.924, a1=64, a2=100\n",
      ">229, d1=0.736, d2=0.552 g=2.282, a1=59, a2=71\n",
      ">230, d1=0.695, d2=0.255 g=1.787, a1=64, a2=100\n",
      ">231, d1=0.416, d2=0.495 g=1.908, a1=78, a2=100\n",
      ">232, d1=0.581, d2=0.626 g=1.512, a1=70, a2=70\n",
      ">233, d1=0.781, d2=0.873 g=1.604, a1=59, a2=42\n",
      ">234, d1=1.145, d2=0.883 g=1.826, a1=40, a2=54\n",
      ">235, d1=0.846, d2=0.410 g=1.749, a1=56, a2=100\n",
      ">236, d1=0.605, d2=0.415 g=1.354, a1=68, a2=100\n",
      ">237, d1=0.327, d2=0.519 g=1.860, a1=89, a2=73\n",
      ">238, d1=0.659, d2=0.578 g=1.390, a1=60, a2=48\n",
      ">239, d1=0.685, d2=1.192 g=1.528, a1=60, a2=28\n",
      ">240, d1=0.920, d2=0.760 g=1.394, a1=45, a2=53\n",
      ">241, d1=1.176, d2=0.626 g=1.404, a1=42, a2=48\n",
      ">242, d1=0.621, d2=0.418 g=1.405, a1=67, a2=100\n",
      ">243, d1=0.532, d2=0.350 g=1.079, a1=71, a2=100\n",
      ">244, d1=0.310, d2=0.318 g=1.118, a1=87, a2=100\n",
      ">245, d1=0.423, d2=0.473 g=0.877, a1=79, a2=100\n",
      ">246, d1=0.484, d2=0.214 g=0.477, a1=78, a2=100\n",
      ">247, d1=0.329, d2=0.365 g=0.475, a1=93, a2=100\n",
      ">248, d1=0.398, d2=0.271 g=0.397, a1=81, a2=100\n",
      ">249, d1=0.479, d2=0.422 g=0.369, a1=84, a2=100\n",
      ">250, d1=0.467, d2=0.432 g=0.366, a1=81, a2=96\n",
      ">251, d1=0.650, d2=0.408 g=0.124, a1=64, a2=79\n",
      ">252, d1=0.375, d2=0.593 g=0.225, a1=85, a2=57\n",
      ">253, d1=0.889, d2=0.627 g=0.149, a1=48, a2=62\n",
      ">254, d1=0.355, d2=0.348 g=0.230, a1=85, a2=100\n",
      ">255, d1=0.709, d2=0.973 g=0.528, a1=51, a2=26\n",
      ">256, d1=0.555, d2=0.412 g=1.087, a1=70, a2=82\n",
      ">257, d1=0.624, d2=0.506 g=1.516, a1=62, a2=75\n",
      ">258, d1=0.472, d2=0.281 g=1.429, a1=73, a2=100\n",
      ">259, d1=0.295, d2=0.340 g=1.796, a1=95, a2=100\n",
      ">260, d1=0.289, d2=0.190 g=2.237, a1=90, a2=100\n",
      ">261, d1=0.256, d2=0.160 g=1.912, a1=96, a2=100\n",
      ">262, d1=0.180, d2=0.341 g=1.609, a1=95, a2=100\n",
      ">263, d1=0.327, d2=0.427 g=1.548, a1=87, a2=100\n",
      ">264, d1=0.429, d2=0.447 g=1.648, a1=82, a2=100\n",
      ">265, d1=0.476, d2=0.318 g=1.838, a1=78, a2=100\n",
      ">266, d1=0.575, d2=0.369 g=1.472, a1=70, a2=100\n",
      ">267, d1=0.338, d2=0.415 g=1.597, a1=82, a2=100\n",
      ">268, d1=0.409, d2=0.343 g=1.324, a1=81, a2=100\n",
      ">269, d1=0.541, d2=0.725 g=1.341, a1=68, a2=40\n",
      ">270, d1=1.037, d2=0.811 g=1.066, a1=46, a2=0\n",
      ">271, d1=1.106, d2=0.885 g=0.983, a1=42, a2=26\n",
      ">272, d1=0.958, d2=0.681 g=0.983, a1=43, a2=46\n",
      ">273, d1=1.084, d2=0.708 g=0.992, a1=35, a2=54\n",
      ">274, d1=0.880, d2=0.464 g=1.166, a1=46, a2=95\n",
      ">275, d1=0.822, d2=0.589 g=1.358, a1=53, a2=65\n",
      ">276, d1=0.647, d2=0.407 g=1.488, a1=65, a2=84\n",
      ">277, d1=0.449, d2=0.239 g=1.578, a1=78, a2=100\n",
      ">278, d1=0.377, d2=0.281 g=1.613, a1=82, a2=100\n",
      ">279, d1=0.326, d2=0.260 g=1.795, a1=92, a2=100\n",
      ">280, d1=0.280, d2=0.214 g=1.882, a1=90, a2=100\n",
      ">281, d1=0.303, d2=0.298 g=2.001, a1=89, a2=100\n",
      ">282, d1=0.297, d2=0.222 g=2.133, a1=93, a2=100\n",
      ">283, d1=0.323, d2=0.247 g=2.131, a1=89, a2=100\n",
      ">284, d1=0.225, d2=0.190 g=2.517, a1=93, a2=100\n",
      ">285, d1=0.222, d2=0.176 g=2.459, a1=95, a2=100\n",
      ">286, d1=0.244, d2=0.262 g=2.216, a1=90, a2=100\n",
      ">287, d1=0.187, d2=0.211 g=2.253, a1=95, a2=100\n",
      ">288, d1=0.233, d2=0.209 g=1.472, a1=93, a2=100\n",
      ">289, d1=0.152, d2=0.302 g=0.680, a1=95, a2=100\n",
      ">290, d1=0.319, d2=0.508 g=0.393, a1=84, a2=65\n",
      ">291, d1=0.718, d2=0.399 g=0.227, a1=67, a2=100\n",
      ">292, d1=0.190, d2=0.130 g=0.199, a1=93, a2=100\n",
      ">293, d1=0.216, d2=0.338 g=0.184, a1=92, a2=90\n",
      ">294, d1=0.222, d2=0.255 g=0.281, a1=92, a2=100\n",
      ">295, d1=0.166, d2=0.144 g=0.264, a1=96, a2=100\n",
      ">296, d1=0.175, d2=0.243 g=0.411, a1=96, a2=100\n",
      ">297, d1=0.189, d2=0.178 g=0.672, a1=96, a2=100\n",
      ">298, d1=0.230, d2=0.158 g=0.762, a1=90, a2=100\n",
      ">299, d1=0.214, d2=0.143 g=0.818, a1=93, a2=100\n",
      ">300, d1=0.145, d2=0.095 g=0.918, a1=98, a2=100\n",
      ">301, d1=0.098, d2=0.169 g=1.299, a1=100, a2=100\n",
      ">302, d1=0.128, d2=0.149 g=1.622, a1=100, a2=100\n",
      ">303, d1=0.149, d2=0.087 g=1.837, a1=96, a2=100\n",
      ">304, d1=0.100, d2=0.054 g=1.826, a1=100, a2=100\n",
      ">305, d1=0.098, d2=0.074 g=1.730, a1=100, a2=100\n",
      ">306, d1=0.076, d2=0.103 g=1.953, a1=100, a2=100\n",
      ">307, d1=0.104, d2=0.069 g=2.260, a1=98, a2=100\n",
      ">308, d1=0.100, d2=0.074 g=2.373, a1=98, a2=100\n",
      ">309, d1=0.082, d2=0.104 g=2.200, a1=98, a2=100\n",
      ">310, d1=0.091, d2=0.110 g=2.513, a1=100, a2=100\n",
      ">311, d1=0.113, d2=0.122 g=2.339, a1=98, a2=100\n",
      ">312, d1=0.109, d2=0.117 g=2.298, a1=100, a2=100\n",
      ">313, d1=0.125, d2=0.236 g=2.240, a1=100, a2=100\n",
      ">314, d1=0.265, d2=0.245 g=2.260, a1=89, a2=100\n",
      ">315, d1=0.208, d2=0.175 g=2.184, a1=96, a2=100\n",
      ">316, d1=0.258, d2=0.324 g=2.434, a1=98, a2=100\n",
      ">317, d1=0.299, d2=0.203 g=2.364, a1=95, a2=100\n",
      ">318, d1=0.241, d2=0.194 g=2.399, a1=92, a2=100\n",
      ">319, d1=0.217, d2=0.194 g=2.259, a1=92, a2=100\n",
      ">320, d1=0.206, d2=0.257 g=2.141, a1=92, a2=100\n",
      ">321, d1=0.256, d2=0.354 g=2.278, a1=92, a2=81\n",
      ">322, d1=0.363, d2=0.297 g=1.961, a1=89, a2=100\n",
      ">323, d1=0.349, d2=0.429 g=2.240, a1=87, a2=100\n",
      ">324, d1=0.511, d2=0.294 g=1.840, a1=79, a2=100\n",
      ">325, d1=0.428, d2=0.304 g=1.153, a1=82, a2=100\n",
      ">326, d1=0.322, d2=0.528 g=1.211, a1=89, a2=73\n",
      ">327, d1=0.536, d2=0.605 g=1.876, a1=75, a2=43\n",
      ">328, d1=0.643, d2=0.388 g=1.915, a1=64, a2=100\n",
      ">329, d1=0.671, d2=0.684 g=2.050, a1=70, a2=48\n",
      ">330, d1=0.658, d2=0.387 g=1.860, a1=68, a2=100\n",
      ">331, d1=0.457, d2=0.387 g=1.561, a1=76, a2=100\n",
      ">332, d1=0.367, d2=0.485 g=2.007, a1=87, a2=100\n",
      ">333, d1=0.825, d2=0.683 g=1.705, a1=59, a2=60\n",
      ">334, d1=0.574, d2=0.459 g=1.669, a1=67, a2=93\n",
      ">335, d1=0.475, d2=0.370 g=1.761, a1=76, a2=96\n",
      ">336, d1=0.566, d2=0.262 g=1.180, a1=73, a2=100\n",
      ">337, d1=0.404, d2=0.469 g=1.503, a1=85, a2=100\n",
      ">338, d1=0.519, d2=0.293 g=1.355, a1=75, a2=100\n",
      ">339, d1=0.373, d2=0.515 g=1.575, a1=85, a2=96\n",
      ">340, d1=0.818, d2=0.414 g=1.230, a1=56, a2=100\n",
      ">341, d1=0.509, d2=0.457 g=1.411, a1=75, a2=96\n",
      ">342, d1=0.572, d2=0.560 g=1.639, a1=75, a2=87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">343, d1=0.559, d2=0.510 g=2.164, a1=71, a2=70\n",
      ">344, d1=0.601, d2=0.317 g=2.080, a1=70, a2=100\n",
      ">345, d1=0.524, d2=0.400 g=2.106, a1=73, a2=84\n",
      ">346, d1=0.268, d2=0.127 g=1.704, a1=90, a2=100\n",
      ">347, d1=0.300, d2=0.217 g=1.000, a1=92, a2=100\n",
      ">348, d1=0.209, d2=0.265 g=0.963, a1=92, a2=100\n",
      ">349, d1=0.312, d2=0.278 g=1.076, a1=89, a2=100\n",
      ">350, d1=0.441, d2=0.289 g=0.898, a1=76, a2=100\n",
      ">351, d1=0.287, d2=0.249 g=0.816, a1=84, a2=100\n",
      ">352, d1=0.326, d2=0.328 g=0.931, a1=89, a2=100\n",
      ">353, d1=0.255, d2=0.216 g=0.944, a1=92, a2=100\n",
      ">354, d1=0.421, d2=0.387 g=0.989, a1=78, a2=100\n",
      ">355, d1=0.310, d2=0.339 g=1.381, a1=87, a2=100\n",
      ">356, d1=0.540, d2=0.379 g=1.536, a1=73, a2=100\n",
      ">357, d1=0.391, d2=0.250 g=1.650, a1=78, a2=100\n",
      ">358, d1=0.373, d2=0.303 g=1.791, a1=81, a2=100\n",
      ">359, d1=0.428, d2=0.345 g=2.203, a1=81, a2=100\n",
      ">360, d1=0.367, d2=0.168 g=2.003, a1=84, a2=100\n",
      ">361, d1=0.295, d2=0.220 g=2.074, a1=90, a2=100\n",
      ">362, d1=0.172, d2=0.143 g=2.406, a1=95, a2=100\n",
      ">363, d1=0.363, d2=0.211 g=2.156, a1=85, a2=100\n",
      ">364, d1=0.182, d2=0.221 g=2.485, a1=93, a2=100\n",
      ">365, d1=0.154, d2=0.126 g=2.671, a1=96, a2=100\n",
      ">366, d1=0.243, d2=0.208 g=1.778, a1=89, a2=100\n",
      ">367, d1=0.265, d2=0.691 g=1.883, a1=89, a2=53\n",
      ">368, d1=0.698, d2=0.374 g=1.008, a1=68, a2=87\n",
      ">369, d1=0.726, d2=0.745 g=1.317, a1=67, a2=39\n",
      ">370, d1=0.413, d2=0.076 g=1.146, a1=87, a2=100\n",
      ">371, d1=0.463, d2=0.392 g=0.691, a1=81, a2=87\n",
      ">372, d1=0.198, d2=0.292 g=0.941, a1=93, a2=100\n",
      ">373, d1=0.409, d2=0.431 g=0.957, a1=85, a2=100\n",
      ">374, d1=0.663, d2=0.791 g=1.319, a1=65, a2=35\n",
      ">375, d1=0.782, d2=0.470 g=1.498, a1=60, a2=81\n",
      ">376, d1=0.554, d2=0.338 g=1.617, a1=70, a2=100\n",
      ">377, d1=0.523, d2=0.326 g=1.644, a1=73, a2=100\n",
      ">378, d1=0.440, d2=0.311 g=1.499, a1=81, a2=100\n",
      ">379, d1=0.310, d2=0.275 g=1.530, a1=87, a2=100\n",
      ">380, d1=0.330, d2=0.251 g=1.217, a1=89, a2=100\n",
      ">381, d1=0.286, d2=0.362 g=1.491, a1=84, a2=95\n",
      ">382, d1=0.364, d2=0.338 g=1.677, a1=84, a2=89\n",
      ">383, d1=0.339, d2=0.295 g=1.682, a1=87, a2=96\n",
      ">384, d1=0.311, d2=0.338 g=2.055, a1=85, a2=96\n",
      ">385, d1=0.453, d2=0.322 g=2.272, a1=85, a2=100\n",
      ">386, d1=0.418, d2=0.217 g=2.195, a1=81, a2=100\n",
      ">387, d1=0.252, d2=0.204 g=2.152, a1=93, a2=100\n",
      ">388, d1=0.227, d2=0.242 g=1.931, a1=92, a2=100\n",
      ">389, d1=0.228, d2=0.233 g=1.941, a1=92, a2=100\n",
      ">390, d1=0.242, d2=0.408 g=1.745, a1=93, a2=100\n",
      ">391, d1=0.461, d2=0.693 g=2.233, a1=78, a2=59\n",
      ">392, d1=0.985, d2=0.361 g=1.718, a1=45, a2=100\n",
      ">393, d1=0.615, d2=0.575 g=1.961, a1=70, a2=76\n",
      ">394, d1=0.614, d2=0.276 g=2.250, a1=68, a2=100\n",
      ">395, d1=0.472, d2=0.364 g=2.107, a1=78, a2=82\n",
      ">396, d1=0.430, d2=0.364 g=2.315, a1=78, a2=82\n",
      ">397, d1=0.486, d2=0.242 g=2.008, a1=73, a2=93\n",
      ">398, d1=0.267, d2=0.273 g=2.010, a1=90, a2=82\n",
      ">399, d1=0.355, d2=0.619 g=2.274, a1=82, a2=59\n",
      ">400, d1=0.415, d2=0.278 g=1.775, a1=78, a2=100\n",
      ">401, d1=0.490, d2=0.433 g=1.155, a1=65, a2=76\n",
      ">402, d1=0.318, d2=0.393 g=1.034, a1=87, a2=100\n",
      ">403, d1=0.376, d2=0.408 g=0.868, a1=82, a2=100\n",
      ">404, d1=0.436, d2=0.508 g=0.847, a1=70, a2=78\n",
      ">405, d1=0.923, d2=0.761 g=0.797, a1=46, a2=43\n",
      ">406, d1=0.676, d2=0.528 g=0.976, a1=56, a2=78\n",
      ">407, d1=0.646, d2=0.517 g=1.140, a1=56, a2=84\n",
      ">408, d1=0.547, d2=0.344 g=1.242, a1=76, a2=100\n",
      ">409, d1=0.568, d2=0.444 g=1.154, a1=67, a2=92\n",
      ">410, d1=0.351, d2=0.330 g=1.442, a1=89, a2=100\n",
      ">411, d1=0.474, d2=0.269 g=1.435, a1=75, a2=100\n",
      ">412, d1=0.435, d2=0.342 g=1.471, a1=71, a2=100\n",
      ">413, d1=0.264, d2=0.228 g=1.618, a1=89, a2=100\n",
      ">414, d1=0.381, d2=0.233 g=1.414, a1=82, a2=100\n",
      ">415, d1=0.242, d2=0.345 g=1.864, a1=92, a2=100\n",
      ">416, d1=0.311, d2=0.231 g=2.239, a1=82, a2=100\n",
      ">417, d1=0.195, d2=0.151 g=2.307, a1=96, a2=100\n",
      ">418, d1=0.279, d2=0.270 g=2.064, a1=89, a2=100\n",
      ">419, d1=0.309, d2=0.286 g=1.935, a1=82, a2=100\n",
      ">420, d1=0.271, d2=0.232 g=1.696, a1=93, a2=100\n",
      ">421, d1=0.173, d2=0.369 g=1.784, a1=93, a2=82\n",
      ">422, d1=0.346, d2=0.281 g=1.411, a1=82, a2=100\n",
      ">423, d1=0.394, d2=0.390 g=1.379, a1=81, a2=100\n",
      ">424, d1=0.404, d2=0.208 g=1.068, a1=82, a2=100\n",
      ">425, d1=0.485, d2=0.737 g=1.229, a1=75, a2=35\n",
      ">426, d1=0.477, d2=0.252 g=1.318, a1=73, a2=100\n",
      ">427, d1=0.570, d2=0.628 g=1.357, a1=67, a2=46\n",
      ">428, d1=0.504, d2=0.328 g=1.128, a1=75, a2=100\n",
      ">429, d1=0.518, d2=0.581 g=1.155, a1=68, a2=92\n",
      ">430, d1=0.503, d2=0.433 g=1.419, a1=73, a2=100\n",
      ">431, d1=0.550, d2=0.665 g=1.697, a1=73, a2=64\n",
      ">432, d1=0.656, d2=0.521 g=1.563, a1=62, a2=81\n",
      ">433, d1=0.800, d2=0.999 g=1.981, a1=53, a2=21\n",
      ">434, d1=1.133, d2=0.721 g=1.676, a1=35, a2=53\n",
      ">435, d1=0.909, d2=0.667 g=1.599, a1=45, a2=59\n",
      ">436, d1=0.993, d2=0.903 g=1.803, a1=42, a2=42\n",
      ">437, d1=0.800, d2=0.390 g=1.784, a1=56, a2=100\n",
      ">438, d1=0.724, d2=0.460 g=1.784, a1=70, a2=87\n",
      ">439, d1=0.661, d2=0.415 g=1.693, a1=65, a2=87\n",
      ">440, d1=0.499, d2=0.505 g=1.698, a1=76, a2=71\n",
      ">441, d1=0.380, d2=0.367 g=1.730, a1=84, a2=98\n",
      ">442, d1=0.740, d2=0.601 g=1.679, a1=60, a2=65\n",
      ">443, d1=0.650, d2=0.542 g=1.642, a1=68, a2=71\n",
      ">444, d1=0.699, d2=0.571 g=1.781, a1=56, a2=73\n",
      ">445, d1=0.516, d2=0.386 g=1.379, a1=76, a2=90\n",
      ">446, d1=0.628, d2=0.689 g=1.416, a1=70, a2=48\n",
      ">447, d1=0.683, d2=0.498 g=1.306, a1=60, a2=89\n",
      ">448, d1=0.702, d2=0.573 g=1.464, a1=64, a2=60\n",
      ">449, d1=0.545, d2=0.484 g=1.477, a1=75, a2=100\n",
      ">450, d1=0.626, d2=0.490 g=1.393, a1=60, a2=100\n"
     ]
    }
   ],
   "source": [
    "# example of training an unstable gan for generating a handwritten digit\n",
    "from os import makedirs\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(28,28,1)):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    # downsample to 14x14\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init,\n",
    "    input_shape=in_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample to 7x7\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    model.add(Dense(n_nodes, kernel_initializer=init, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    # upsample to 14x14\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same',\n",
    "    kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 28x28\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same',\n",
    "    kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # output 28x28x1\n",
    "    model.add(Conv2D(1, (7,7), activation='tanh', padding='same', kernel_initializer=init))\n",
    "    return model\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "# load mnist images\n",
    "def load_real_samples():\n",
    "    # load dataset\n",
    "    (trainX, trainy), (_, _) = load_data()\n",
    "    # expand to 3d, e.g. add channels\n",
    "    X = expand_dims(trainX, axis=-1)\n",
    "    # select all of the examples for a given class\n",
    "    selected_ix = trainy == 8\n",
    "    X = X[selected_ix]\n",
    "    # convert from ints to floats\n",
    "    X = X.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    return X\n",
    "\n",
    "# # select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # select images\n",
    "    X = dataset[ix]\n",
    "    # generate class labels\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, latent_dim, n_samples=100):\n",
    "    # prepare fake examples\n",
    "    X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    X = (X + 1) / 2.0\n",
    "    # plot images\n",
    "    for i in range(10 * 10):\n",
    "        # define subplot\n",
    "        pyplot.subplot(10, 10, 1 + i)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
    "    # save plot to file\n",
    "    pyplot.savefig('results_collapse/generated_plot_%03d.png' % (step+1))\n",
    "    pyplot.close()\n",
    "    # save the generator model\n",
    "    g_model.save('results_collapse/model_%03d.h5' % (step+1))\n",
    "    \n",
    "# create a line plot of loss for the gan and save to file\n",
    "def plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist):\n",
    "    # plot loss\n",
    "    pyplot.subplot(2, 1, 1)\n",
    "    pyplot.plot(d1_hist, label='d-real')\n",
    "    pyplot.plot(d2_hist, label='d-fake')\n",
    "    pyplot.plot(g_hist, label='gen')\n",
    "    pyplot.legend()\n",
    "    # plot discriminator accuracy\n",
    "    pyplot.subplot(2, 1, 2)\n",
    "    pyplot.plot(a1_hist, label='acc-real')\n",
    "    pyplot.plot(a2_hist, label='acc-fake')\n",
    "    pyplot.legend()\n",
    "    # save plot to file\n",
    "    pyplot.savefig('results_collapse/plot_line_plot_loss.png')\n",
    "    pyplot.close()\n",
    "    \n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=10, n_batch=128):\n",
    "    # calculate the number of batches per epoch\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    # calculate the total iterations based on batch and epoch\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    # calculate the number of samples in half a batch\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # prepare lists for storing stats each iteration\n",
    "    d1_hist, d2_hist, g_hist, a1_hist, a2_hist = list(), list(), list(), list(), list()\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        # get randomly selected ✬real✬ samples\n",
    "        X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "        # update discriminator model weights\n",
    "        d_loss1, d_acc1 = d_model.train_on_batch(X_real, y_real)\n",
    "        # generate ✬fake✬ examples\n",
    "        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        # update discriminator model weights\n",
    "        d_loss2, d_acc2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "        # prepare points in latent space as input for the generator\n",
    "        X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = ones((n_batch, 1))\n",
    "        # update the generator via the discriminator✬s error\n",
    "        for j in range(2): # Generator updated twice per each discriminator update (to avoid convergence failure)\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "        # summarize loss on this batch\n",
    "        print('>%d, d1=%.3f, d2=%.3f g=%.3f, a1=%d, a2=%d' %\n",
    "                    (i+1, d_loss1, d_loss2, g_loss, int(100*d_acc1), int(100*d_acc2)))\n",
    "        # record history\n",
    "        d1_hist.append(d_loss1)\n",
    "        d2_hist.append(d_loss2)\n",
    "        g_hist.append(g_loss)\n",
    "        a1_hist.append(d_acc1)\n",
    "        a2_hist.append(d_acc2)\n",
    "        # evaluate the model performance every ✬epoch✬\n",
    "        if (i+1) % bat_per_epo == 0:\n",
    "            summarize_performance(i, g_model, latent_dim)\n",
    "    plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist)\n",
    "\n",
    "# make folder for results\n",
    "makedirs('results_collapse', exist_ok=True)\n",
    "# size of the latent space\n",
    "latent_dim = 1\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# load image data\n",
    "dataset = load_real_samples()\n",
    "print(dataset.shape)\n",
    "# train model\n",
    "train(generator, discriminator, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
