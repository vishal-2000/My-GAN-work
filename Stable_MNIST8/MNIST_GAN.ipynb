{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5851, 28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, d1=0.749, d2=0.697 g=0.171, a1=56, a2=53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2, d1=0.182, d2=1.142 g=0.074, a1=98, a2=0\n",
      ">3, d1=0.091, d2=1.419 g=0.062, a1=100, a2=0\n",
      ">4, d1=0.078, d2=1.337 g=0.062, a1=100, a2=0\n",
      ">5, d1=0.077, d2=1.259 g=0.077, a1=100, a2=0\n",
      ">6, d1=0.055, d2=1.111 g=0.083, a1=100, a2=1\n",
      ">7, d1=0.065, d2=1.097 g=0.072, a1=100, a2=4\n",
      ">8, d1=0.068, d2=1.182 g=0.060, a1=100, a2=0\n",
      ">9, d1=0.107, d2=1.266 g=0.059, a1=98, a2=0\n",
      ">10, d1=0.112, d2=1.220 g=0.059, a1=98, a2=0\n",
      ">11, d1=0.109, d2=1.187 g=0.063, a1=100, a2=0\n",
      ">12, d1=0.135, d2=1.074 g=0.064, a1=96, a2=3\n",
      ">13, d1=0.141, d2=0.849 g=0.061, a1=96, a2=23\n",
      ">14, d1=0.136, d2=0.762 g=0.061, a1=98, a2=31\n",
      ">15, d1=0.160, d2=0.704 g=0.060, a1=96, a2=50\n",
      ">16, d1=0.097, d2=0.639 g=0.055, a1=100, a2=64\n",
      ">17, d1=0.123, d2=0.628 g=0.058, a1=100, a2=67\n",
      ">18, d1=0.190, d2=0.576 g=0.062, a1=95, a2=78\n",
      ">19, d1=0.134, d2=0.509 g=0.069, a1=98, a2=93\n",
      ">20, d1=0.107, d2=0.510 g=0.081, a1=100, a2=92\n",
      ">21, d1=0.167, d2=0.554 g=0.093, a1=96, a2=84\n",
      ">22, d1=0.104, d2=0.439 g=0.117, a1=98, a2=96\n",
      ">23, d1=0.145, d2=0.407 g=0.170, a1=98, a2=93\n",
      ">24, d1=0.078, d2=0.341 g=0.247, a1=100, a2=100\n",
      ">25, d1=0.136, d2=0.264 g=0.491, a1=96, a2=100\n",
      ">26, d1=0.131, d2=0.337 g=0.589, a1=95, a2=100\n",
      ">27, d1=0.092, d2=0.156 g=0.665, a1=95, a2=100\n",
      ">28, d1=0.107, d2=0.180 g=0.526, a1=96, a2=100\n",
      ">29, d1=0.090, d2=0.154 g=0.378, a1=100, a2=100\n",
      ">30, d1=0.057, d2=0.127 g=0.412, a1=100, a2=100\n",
      ">31, d1=0.058, d2=0.119 g=0.428, a1=100, a2=100\n",
      ">32, d1=0.057, d2=0.172 g=0.419, a1=100, a2=100\n",
      ">33, d1=0.066, d2=0.405 g=0.251, a1=100, a2=93\n",
      ">34, d1=0.150, d2=0.762 g=0.291, a1=96, a2=37\n",
      ">35, d1=0.196, d2=0.143 g=0.213, a1=96, a2=100\n",
      ">36, d1=0.149, d2=0.122 g=0.120, a1=95, a2=100\n",
      ">37, d1=0.082, d2=0.075 g=0.086, a1=98, a2=100\n",
      ">38, d1=0.104, d2=0.046 g=0.059, a1=98, a2=100\n",
      ">39, d1=0.083, d2=0.039 g=0.048, a1=98, a2=100\n",
      ">40, d1=0.071, d2=0.026 g=0.032, a1=98, a2=100\n",
      ">41, d1=0.045, d2=0.017 g=0.030, a1=100, a2=100\n",
      ">42, d1=0.041, d2=0.015 g=0.023, a1=100, a2=100\n",
      ">43, d1=0.031, d2=0.012 g=0.016, a1=100, a2=100\n",
      ">44, d1=0.031, d2=0.010 g=0.015, a1=100, a2=100\n",
      ">45, d1=0.018, d2=0.009 g=0.014, a1=100, a2=100\n",
      ">46, d1=0.025, d2=0.008 g=0.013, a1=100, a2=100\n",
      ">47, d1=0.016, d2=0.008 g=0.010, a1=100, a2=100\n",
      ">48, d1=0.027, d2=0.006 g=0.009, a1=100, a2=100\n",
      ">49, d1=0.017, d2=0.005 g=0.007, a1=100, a2=100\n",
      ">50, d1=0.017, d2=0.005 g=0.007, a1=100, a2=100\n",
      ">51, d1=0.016, d2=0.005 g=0.006, a1=100, a2=100\n",
      ">52, d1=0.014, d2=0.004 g=0.005, a1=100, a2=100\n",
      ">53, d1=0.014, d2=0.004 g=0.005, a1=100, a2=100\n",
      ">54, d1=0.018, d2=0.004 g=0.004, a1=100, a2=100\n",
      ">55, d1=0.023, d2=0.003 g=0.004, a1=100, a2=100\n",
      ">56, d1=0.008, d2=0.003 g=0.004, a1=100, a2=100\n",
      ">57, d1=0.010, d2=0.003 g=0.003, a1=100, a2=100\n",
      ">58, d1=0.012, d2=0.003 g=0.003, a1=100, a2=100\n",
      ">59, d1=0.011, d2=0.003 g=0.003, a1=100, a2=100\n",
      ">60, d1=0.010, d2=0.002 g=0.003, a1=100, a2=100\n",
      ">61, d1=0.017, d2=0.002 g=0.003, a1=100, a2=100\n",
      ">62, d1=0.023, d2=0.003 g=0.003, a1=100, a2=100\n",
      ">63, d1=0.015, d2=0.002 g=0.002, a1=100, a2=100\n",
      ">64, d1=0.009, d2=0.002 g=0.002, a1=100, a2=100\n",
      ">65, d1=0.006, d2=0.002 g=0.002, a1=100, a2=100\n",
      ">66, d1=0.007, d2=0.003 g=0.002, a1=100, a2=100\n",
      ">67, d1=0.006, d2=0.003 g=0.002, a1=100, a2=100\n",
      ">68, d1=0.007, d2=0.003 g=0.002, a1=100, a2=100\n",
      ">69, d1=0.009, d2=0.002 g=0.002, a1=100, a2=100\n",
      ">70, d1=0.008, d2=0.002 g=0.002, a1=100, a2=100\n",
      ">71, d1=0.006, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">72, d1=0.005, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">73, d1=0.005, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">74, d1=0.010, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">75, d1=0.005, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">76, d1=0.005, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">77, d1=0.005, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">78, d1=0.004, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">79, d1=0.006, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">80, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">81, d1=0.006, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">82, d1=0.007, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">83, d1=0.004, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">84, d1=0.006, d2=0.001 g=0.002, a1=100, a2=100\n",
      ">85, d1=0.004, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">86, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">87, d1=0.006, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">88, d1=0.003, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">89, d1=0.005, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">90, d1=0.011, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">91, d1=0.004, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">92, d1=0.004, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">93, d1=0.005, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">94, d1=0.007, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">95, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">96, d1=0.005, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">97, d1=0.003, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">98, d1=0.004, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">99, d1=0.002, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">100, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">101, d1=0.003, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">102, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">103, d1=0.003, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">104, d1=0.005, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">105, d1=0.003, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">106, d1=0.003, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">107, d1=0.007, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">108, d1=0.002, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">109, d1=0.004, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">110, d1=0.005, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">111, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">112, d1=0.004, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">113, d1=0.004, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">114, d1=0.002, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">115, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">116, d1=0.005, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">117, d1=0.004, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">118, d1=0.002, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">119, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">120, d1=0.004, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">121, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">122, d1=0.005, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">123, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">124, d1=0.005, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">125, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">126, d1=0.002, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">127, d1=0.002, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">128, d1=0.003, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">129, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">130, d1=0.002, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">131, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">132, d1=0.001, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">133, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">134, d1=0.005, d2=0.001 g=0.001, a1=100, a2=100\n",
      ">135, d1=0.003, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">136, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">137, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">138, d1=0.003, d2=0.002 g=0.000, a1=100, a2=100\n",
      ">139, d1=0.004, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">140, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">141, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">142, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">143, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">144, d1=0.005, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">145, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">146, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">147, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">148, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">149, d1=0.002, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">150, d1=0.003, d2=0.004 g=0.001, a1=100, a2=100\n",
      ">151, d1=0.002, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">152, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">153, d1=0.007, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">154, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">155, d1=0.005, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">156, d1=0.003, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">157, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">158, d1=0.003, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">159, d1=0.003, d2=0.004 g=0.001, a1=100, a2=100\n",
      ">160, d1=0.006, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">161, d1=0.002, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">162, d1=0.003, d2=0.004 g=0.001, a1=100, a2=100\n",
      ">163, d1=0.003, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">164, d1=0.003, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">165, d1=0.005, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">166, d1=0.003, d2=0.004 g=0.001, a1=100, a2=100\n",
      ">167, d1=0.004, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">168, d1=0.002, d2=0.004 g=0.001, a1=100, a2=100\n",
      ">169, d1=0.005, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">170, d1=0.004, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">171, d1=0.006, d2=0.004 g=0.001, a1=100, a2=100\n",
      ">172, d1=0.004, d2=0.006 g=0.001, a1=100, a2=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">173, d1=0.004, d2=0.003 g=0.001, a1=100, a2=100\n",
      ">174, d1=0.004, d2=0.002 g=0.001, a1=100, a2=100\n",
      ">175, d1=0.007, d2=0.004 g=0.001, a1=100, a2=100\n",
      ">176, d1=0.003, d2=0.005 g=0.002, a1=100, a2=100\n",
      ">177, d1=0.005, d2=0.002 g=0.002, a1=100, a2=100\n",
      ">178, d1=0.007, d2=0.005 g=0.002, a1=100, a2=100\n",
      ">179, d1=0.005, d2=0.008 g=0.002, a1=100, a2=100\n",
      ">180, d1=0.004, d2=0.004 g=0.002, a1=100, a2=100\n",
      ">181, d1=0.005, d2=0.004 g=0.002, a1=100, a2=100\n",
      ">182, d1=0.007, d2=0.005 g=0.002, a1=100, a2=100\n",
      ">183, d1=0.006, d2=0.007 g=0.002, a1=100, a2=100\n",
      ">184, d1=0.005, d2=0.007 g=0.003, a1=100, a2=100\n",
      ">185, d1=0.005, d2=0.004 g=0.003, a1=100, a2=100\n",
      ">186, d1=0.014, d2=0.007 g=0.004, a1=100, a2=100\n",
      ">187, d1=0.008, d2=0.004 g=0.003, a1=100, a2=100\n",
      ">188, d1=0.007, d2=0.005 g=0.003, a1=100, a2=100\n",
      ">189, d1=0.015, d2=0.005 g=0.003, a1=100, a2=100\n",
      ">190, d1=0.006, d2=0.014 g=0.005, a1=100, a2=100\n",
      ">191, d1=0.007, d2=0.003 g=0.007, a1=100, a2=100\n",
      ">192, d1=0.009, d2=0.009 g=0.008, a1=100, a2=100\n",
      ">193, d1=0.005, d2=0.006 g=0.009, a1=100, a2=100\n",
      ">194, d1=0.016, d2=0.010 g=0.010, a1=100, a2=100\n",
      ">195, d1=0.011, d2=0.012 g=0.012, a1=100, a2=100\n",
      ">196, d1=0.018, d2=0.012 g=0.015, a1=100, a2=100\n",
      ">197, d1=0.023, d2=0.010 g=0.013, a1=100, a2=100\n",
      ">198, d1=0.011, d2=0.008 g=0.013, a1=100, a2=100\n",
      ">199, d1=0.018, d2=0.007 g=0.011, a1=100, a2=100\n",
      ">200, d1=0.008, d2=0.011 g=0.016, a1=100, a2=100\n",
      ">201, d1=0.018, d2=0.011 g=0.020, a1=100, a2=100\n",
      ">202, d1=0.017, d2=0.013 g=0.021, a1=100, a2=100\n",
      ">203, d1=0.015, d2=0.015 g=0.037, a1=100, a2=100\n",
      ">204, d1=0.013, d2=0.006 g=0.037, a1=100, a2=100\n",
      ">205, d1=0.016, d2=0.012 g=0.037, a1=100, a2=100\n",
      ">206, d1=0.022, d2=0.008 g=0.025, a1=100, a2=100\n",
      ">207, d1=0.015, d2=0.012 g=0.029, a1=100, a2=100\n",
      ">208, d1=0.024, d2=0.007 g=0.024, a1=100, a2=100\n",
      ">209, d1=0.021, d2=0.023 g=0.048, a1=100, a2=100\n",
      ">210, d1=0.017, d2=0.002 g=0.042, a1=100, a2=100\n",
      ">211, d1=0.020, d2=0.008 g=0.019, a1=100, a2=100\n",
      ">212, d1=0.012, d2=0.013 g=0.011, a1=100, a2=100\n",
      ">213, d1=0.010, d2=0.021 g=0.005, a1=100, a2=100\n",
      ">214, d1=0.048, d2=0.812 g=7.554, a1=100, a2=46\n",
      ">215, d1=9.609, d2=0.129 g=1.273, a1=0, a2=100\n",
      ">216, d1=1.389, d2=1.287 g=3.704, a1=12, a2=6\n",
      ">217, d1=0.023, d2=0.059 g=1.019, a1=100, a2=100\n",
      ">218, d1=0.021, d2=0.559 g=2.881, a1=100, a2=71\n",
      ">219, d1=0.045, d2=0.473 g=2.605, a1=100, a2=73\n",
      ">220, d1=0.134, d2=0.096 g=2.534, a1=96, a2=100\n",
      ">221, d1=0.186, d2=0.556 g=2.136, a1=96, a2=67\n",
      ">222, d1=0.301, d2=0.334 g=1.504, a1=93, a2=90\n",
      ">223, d1=0.479, d2=0.976 g=1.704, a1=79, a2=31\n",
      ">224, d1=0.564, d2=0.338 g=1.451, a1=79, a2=95\n",
      ">225, d1=0.464, d2=0.604 g=2.062, a1=82, a2=75\n",
      ">226, d1=0.262, d2=0.110 g=1.978, a1=93, a2=100\n",
      ">227, d1=0.402, d2=0.332 g=2.272, a1=82, a2=93\n",
      ">228, d1=0.154, d2=0.071 g=2.543, a1=98, a2=100\n",
      ">229, d1=0.134, d2=0.065 g=2.056, a1=98, a2=100\n",
      ">230, d1=0.077, d2=0.058 g=1.960, a1=100, a2=100\n",
      ">231, d1=0.060, d2=0.045 g=1.842, a1=100, a2=100\n",
      ">232, d1=0.033, d2=0.047 g=1.872, a1=100, a2=100\n",
      ">233, d1=0.042, d2=0.030 g=1.906, a1=100, a2=100\n",
      ">234, d1=0.034, d2=0.027 g=1.827, a1=100, a2=100\n",
      ">235, d1=0.032, d2=0.022 g=1.593, a1=100, a2=100\n",
      ">236, d1=0.028, d2=0.020 g=1.361, a1=100, a2=100\n",
      ">237, d1=0.021, d2=0.018 g=1.059, a1=100, a2=100\n",
      ">238, d1=0.027, d2=0.045 g=0.792, a1=100, a2=100\n",
      ">239, d1=0.023, d2=0.042 g=0.823, a1=100, a2=100\n",
      ">240, d1=0.033, d2=0.062 g=1.002, a1=100, a2=100\n",
      ">241, d1=0.028, d2=0.037 g=0.944, a1=100, a2=100\n",
      ">242, d1=0.026, d2=0.083 g=0.790, a1=100, a2=100\n",
      ">243, d1=0.032, d2=0.251 g=2.304, a1=100, a2=100\n",
      ">244, d1=0.106, d2=0.188 g=1.119, a1=100, a2=100\n",
      ">245, d1=0.208, d2=1.047 g=5.152, a1=98, a2=12\n",
      ">246, d1=1.554, d2=0.060 g=1.756, a1=0, a2=100\n",
      ">247, d1=0.049, d2=0.655 g=3.281, a1=98, a2=64\n",
      ">248, d1=0.081, d2=0.029 g=2.788, a1=98, a2=100\n",
      ">249, d1=0.157, d2=0.297 g=2.287, a1=100, a2=90\n",
      ">250, d1=0.362, d2=0.274 g=0.999, a1=90, a2=95\n",
      ">251, d1=0.233, d2=1.188 g=2.036, a1=95, a2=23\n",
      ">252, d1=1.350, d2=1.060 g=1.159, a1=9, a2=26\n",
      ">253, d1=0.793, d2=1.072 g=1.128, a1=48, a2=15\n",
      ">254, d1=0.948, d2=0.640 g=0.741, a1=40, a2=62\n",
      ">255, d1=0.730, d2=0.916 g=0.678, a1=48, a2=54\n",
      ">256, d1=0.557, d2=0.725 g=0.743, a1=73, a2=60\n",
      ">257, d1=0.522, d2=0.845 g=1.054, a1=76, a2=53\n",
      ">258, d1=0.790, d2=0.821 g=1.131, a1=54, a2=56\n",
      ">259, d1=0.872, d2=0.884 g=1.084, a1=43, a2=45\n",
      ">260, d1=0.798, d2=0.815 g=1.182, a1=48, a2=50\n",
      ">261, d1=0.829, d2=1.011 g=1.230, a1=48, a2=40\n",
      ">262, d1=0.955, d2=0.854 g=1.092, a1=43, a2=48\n",
      ">263, d1=0.817, d2=0.899 g=1.109, a1=46, a2=45\n",
      ">264, d1=0.840, d2=0.785 g=0.969, a1=43, a2=45\n",
      ">265, d1=0.797, d2=0.885 g=0.856, a1=57, a2=43\n",
      ">266, d1=0.852, d2=1.108 g=0.864, a1=51, a2=28\n",
      ">267, d1=0.853, d2=0.721 g=0.962, a1=48, a2=54\n",
      ">268, d1=0.847, d2=0.796 g=0.872, a1=43, a2=39\n",
      ">269, d1=0.768, d2=0.833 g=0.947, a1=50, a2=35\n",
      ">270, d1=0.700, d2=0.679 g=0.982, a1=53, a2=54\n",
      ">271, d1=0.842, d2=0.796 g=0.951, a1=46, a2=31\n",
      ">272, d1=0.830, d2=0.767 g=0.992, a1=48, a2=46\n",
      ">273, d1=0.828, d2=0.834 g=1.004, a1=43, a2=46\n",
      ">274, d1=0.713, d2=0.763 g=1.088, a1=56, a2=45\n",
      ">275, d1=0.857, d2=0.736 g=1.004, a1=39, a2=53\n",
      ">276, d1=0.702, d2=0.690 g=1.001, a1=56, a2=53\n",
      ">277, d1=0.669, d2=0.756 g=0.996, a1=60, a2=54\n",
      ">278, d1=0.772, d2=0.727 g=0.887, a1=43, a2=53\n",
      ">279, d1=0.805, d2=0.850 g=0.934, a1=56, a2=46\n",
      ">280, d1=0.801, d2=0.809 g=0.906, a1=54, a2=45\n",
      ">281, d1=0.730, d2=0.813 g=0.987, a1=54, a2=50\n",
      ">282, d1=0.940, d2=0.845 g=0.834, a1=35, a2=46\n",
      ">283, d1=0.801, d2=0.848 g=0.839, a1=40, a2=35\n",
      ">284, d1=0.855, d2=0.955 g=0.891, a1=45, a2=21\n",
      ">285, d1=0.934, d2=0.885 g=0.887, a1=28, a2=31\n",
      ">286, d1=0.970, d2=0.868 g=0.867, a1=23, a2=34\n",
      ">287, d1=0.797, d2=0.837 g=0.918, a1=31, a2=35\n",
      ">288, d1=0.864, d2=0.776 g=0.951, a1=32, a2=46\n",
      ">289, d1=0.843, d2=0.826 g=0.892, a1=46, a2=35\n",
      ">290, d1=0.848, d2=0.879 g=0.837, a1=35, a2=28\n",
      ">291, d1=0.820, d2=0.743 g=0.929, a1=40, a2=46\n",
      ">292, d1=0.763, d2=0.770 g=0.997, a1=48, a2=42\n",
      ">293, d1=0.850, d2=0.656 g=0.871, a1=35, a2=64\n",
      ">294, d1=0.813, d2=0.795 g=0.821, a1=39, a2=28\n",
      ">295, d1=0.614, d2=0.746 g=0.879, a1=64, a2=51\n",
      ">296, d1=0.829, d2=0.750 g=0.766, a1=45, a2=48\n",
      ">297, d1=0.755, d2=0.904 g=0.756, a1=53, a2=43\n",
      ">298, d1=0.779, d2=0.860 g=0.762, a1=53, a2=34\n",
      ">299, d1=0.907, d2=0.854 g=0.772, a1=39, a2=26\n",
      ">300, d1=0.869, d2=0.853 g=0.743, a1=35, a2=28\n",
      ">301, d1=0.851, d2=0.866 g=0.835, a1=34, a2=28\n",
      ">302, d1=0.868, d2=0.737 g=0.892, a1=31, a2=53\n",
      ">303, d1=0.853, d2=0.792 g=0.846, a1=34, a2=37\n",
      ">304, d1=0.774, d2=0.762 g=0.958, a1=51, a2=39\n",
      ">305, d1=0.844, d2=0.663 g=0.953, a1=32, a2=68\n",
      ">306, d1=0.793, d2=0.710 g=0.894, a1=37, a2=46\n",
      ">307, d1=0.797, d2=0.753 g=0.826, a1=42, a2=45\n",
      ">308, d1=0.726, d2=0.836 g=0.859, a1=51, a2=29\n",
      ">309, d1=0.710, d2=0.802 g=0.886, a1=53, a2=45\n",
      ">310, d1=0.887, d2=0.828 g=0.827, a1=29, a2=26\n",
      ">311, d1=1.003, d2=0.812 g=0.744, a1=20, a2=37\n",
      ">312, d1=0.787, d2=0.854 g=0.808, a1=45, a2=35\n",
      ">313, d1=0.829, d2=0.895 g=0.775, a1=42, a2=23\n",
      ">314, d1=0.835, d2=0.841 g=0.814, a1=32, a2=29\n",
      ">315, d1=0.890, d2=0.873 g=0.836, a1=34, a2=28\n",
      ">316, d1=0.800, d2=0.776 g=0.887, a1=31, a2=40\n",
      ">317, d1=0.835, d2=0.800 g=0.807, a1=39, a2=37\n",
      ">318, d1=0.842, d2=0.786 g=0.812, a1=32, a2=39\n",
      ">319, d1=0.922, d2=0.922 g=0.770, a1=29, a2=25\n",
      ">320, d1=0.917, d2=0.876 g=0.777, a1=23, a2=31\n",
      ">321, d1=0.897, d2=0.801 g=0.793, a1=26, a2=42\n",
      ">322, d1=0.971, d2=0.834 g=0.752, a1=28, a2=26\n",
      ">323, d1=0.853, d2=0.815 g=0.751, a1=37, a2=25\n",
      ">324, d1=0.905, d2=0.803 g=0.741, a1=31, a2=28\n",
      ">325, d1=0.867, d2=0.785 g=0.696, a1=25, a2=37\n",
      ">326, d1=0.836, d2=0.887 g=0.689, a1=26, a2=21\n",
      ">327, d1=0.880, d2=0.821 g=0.710, a1=26, a2=23\n",
      ">328, d1=0.815, d2=0.817 g=0.733, a1=35, a2=28\n",
      ">329, d1=0.906, d2=0.792 g=0.699, a1=17, a2=28\n",
      ">330, d1=0.911, d2=0.869 g=0.703, a1=18, a2=20\n",
      ">331, d1=0.849, d2=0.834 g=0.727, a1=25, a2=37\n",
      ">332, d1=0.824, d2=0.811 g=0.712, a1=25, a2=28\n",
      ">333, d1=0.848, d2=0.870 g=0.719, a1=21, a2=12\n",
      ">334, d1=0.877, d2=0.879 g=0.724, a1=20, a2=15\n",
      ">335, d1=0.891, d2=0.892 g=0.717, a1=20, a2=15\n",
      ">336, d1=0.813, d2=0.882 g=0.728, a1=26, a2=18\n",
      ">337, d1=0.784, d2=0.851 g=0.773, a1=31, a2=23\n",
      ">338, d1=0.896, d2=0.868 g=0.809, a1=23, a2=23\n",
      ">339, d1=0.813, d2=0.759 g=0.757, a1=39, a2=43\n",
      ">340, d1=0.874, d2=0.844 g=0.762, a1=23, a2=29\n",
      ">341, d1=0.822, d2=0.824 g=0.740, a1=31, a2=34\n",
      ">342, d1=0.836, d2=0.832 g=0.750, a1=34, a2=28\n",
      ">343, d1=0.849, d2=0.823 g=0.797, a1=34, a2=25\n",
      ">344, d1=0.862, d2=0.830 g=0.767, a1=28, a2=32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">345, d1=0.872, d2=0.788 g=0.761, a1=29, a2=37\n",
      ">346, d1=0.869, d2=0.880 g=0.765, a1=26, a2=28\n",
      ">347, d1=0.916, d2=0.858 g=0.727, a1=23, a2=17\n",
      ">348, d1=0.905, d2=0.859 g=0.726, a1=20, a2=15\n",
      ">349, d1=0.835, d2=0.849 g=0.754, a1=26, a2=18\n",
      ">350, d1=0.874, d2=0.816 g=0.739, a1=18, a2=26\n",
      ">351, d1=0.927, d2=0.830 g=0.748, a1=14, a2=25\n",
      ">352, d1=0.862, d2=0.848 g=0.718, a1=25, a2=18\n",
      ">353, d1=0.954, d2=0.855 g=0.725, a1=12, a2=29\n",
      ">354, d1=0.854, d2=0.878 g=0.716, a1=25, a2=26\n",
      ">355, d1=0.807, d2=0.823 g=0.753, a1=26, a2=21\n",
      ">356, d1=0.841, d2=0.815 g=0.758, a1=25, a2=29\n",
      ">357, d1=0.861, d2=0.842 g=0.785, a1=21, a2=18\n",
      ">358, d1=0.880, d2=0.852 g=0.731, a1=17, a2=20\n",
      ">359, d1=0.854, d2=0.841 g=0.737, a1=29, a2=25\n",
      ">360, d1=0.844, d2=0.801 g=0.711, a1=26, a2=25\n",
      ">361, d1=0.873, d2=0.838 g=0.708, a1=21, a2=14\n",
      ">362, d1=0.892, d2=0.831 g=0.707, a1=17, a2=25\n",
      ">363, d1=0.814, d2=0.810 g=0.693, a1=26, a2=18\n",
      ">364, d1=0.824, d2=0.869 g=0.697, a1=23, a2=15\n",
      ">365, d1=0.841, d2=0.872 g=0.716, a1=15, a2=14\n",
      ">366, d1=0.829, d2=0.829 g=0.722, a1=28, a2=25\n",
      ">367, d1=0.862, d2=0.824 g=0.771, a1=25, a2=14\n",
      ">368, d1=0.871, d2=0.786 g=0.747, a1=15, a2=29\n",
      ">369, d1=0.776, d2=0.781 g=0.749, a1=34, a2=23\n",
      ">370, d1=0.868, d2=0.810 g=0.737, a1=21, a2=25\n",
      ">371, d1=0.769, d2=0.812 g=0.761, a1=32, a2=21\n",
      ">372, d1=0.819, d2=0.787 g=0.762, a1=26, a2=21\n",
      ">373, d1=0.807, d2=0.821 g=0.775, a1=37, a2=23\n",
      ">374, d1=0.864, d2=0.770 g=0.802, a1=21, a2=26\n",
      ">375, d1=0.903, d2=0.724 g=0.798, a1=15, a2=45\n",
      ">376, d1=0.872, d2=0.755 g=0.784, a1=20, a2=31\n",
      ">377, d1=0.901, d2=0.821 g=0.731, a1=17, a2=15\n",
      ">378, d1=0.815, d2=0.808 g=0.771, a1=28, a2=15\n",
      ">379, d1=0.847, d2=0.746 g=0.814, a1=18, a2=37\n",
      ">380, d1=0.874, d2=0.797 g=0.762, a1=18, a2=31\n",
      ">381, d1=0.902, d2=0.853 g=0.719, a1=7, a2=7\n",
      ">382, d1=0.821, d2=0.826 g=0.726, a1=14, a2=12\n",
      ">383, d1=0.850, d2=0.787 g=0.743, a1=14, a2=15\n",
      ">384, d1=0.843, d2=0.792 g=0.725, a1=18, a2=23\n",
      ">385, d1=0.825, d2=0.869 g=0.760, a1=15, a2=6\n",
      ">386, d1=0.847, d2=0.835 g=0.757, a1=15, a2=23\n",
      ">387, d1=0.837, d2=0.788 g=0.768, a1=23, a2=14\n",
      ">388, d1=0.839, d2=0.805 g=0.753, a1=20, a2=20\n",
      ">389, d1=0.820, d2=0.765 g=0.775, a1=21, a2=39\n",
      ">390, d1=0.812, d2=0.788 g=0.772, a1=18, a2=21\n",
      ">391, d1=0.791, d2=0.754 g=0.767, a1=32, a2=28\n",
      ">392, d1=0.808, d2=0.812 g=0.738, a1=20, a2=21\n",
      ">393, d1=0.806, d2=0.742 g=0.751, a1=21, a2=32\n",
      ">394, d1=0.797, d2=0.833 g=0.748, a1=28, a2=18\n",
      ">395, d1=0.817, d2=0.786 g=0.761, a1=29, a2=26\n",
      ">396, d1=0.853, d2=0.799 g=0.761, a1=20, a2=18\n",
      ">397, d1=0.782, d2=0.783 g=0.775, a1=34, a2=26\n",
      ">398, d1=0.835, d2=0.786 g=0.770, a1=29, a2=18\n",
      ">399, d1=0.809, d2=0.731 g=0.789, a1=29, a2=43\n",
      ">400, d1=0.785, d2=0.796 g=0.742, a1=28, a2=28\n",
      ">401, d1=0.811, d2=0.776 g=0.752, a1=21, a2=28\n",
      ">402, d1=0.821, d2=0.794 g=0.737, a1=20, a2=25\n",
      ">403, d1=0.791, d2=0.783 g=0.757, a1=31, a2=26\n",
      ">404, d1=0.788, d2=0.747 g=0.755, a1=28, a2=40\n",
      ">405, d1=0.817, d2=0.812 g=0.750, a1=20, a2=17\n",
      ">406, d1=0.839, d2=0.767 g=0.737, a1=25, a2=20\n",
      ">407, d1=0.800, d2=0.796 g=0.716, a1=20, a2=25\n",
      ">408, d1=0.818, d2=0.802 g=0.720, a1=26, a2=31\n",
      ">409, d1=0.776, d2=0.776 g=0.713, a1=23, a2=26\n",
      ">410, d1=0.795, d2=0.797 g=0.720, a1=21, a2=28\n",
      ">411, d1=0.815, d2=0.797 g=0.751, a1=34, a2=20\n",
      ">412, d1=0.806, d2=0.820 g=0.767, a1=23, a2=14\n",
      ">413, d1=0.840, d2=0.724 g=0.789, a1=18, a2=45\n",
      ">414, d1=0.816, d2=0.777 g=0.770, a1=29, a2=31\n",
      ">415, d1=0.818, d2=0.758 g=0.768, a1=18, a2=25\n",
      ">416, d1=0.784, d2=0.797 g=0.748, a1=29, a2=29\n",
      ">417, d1=0.749, d2=0.795 g=0.780, a1=37, a2=20\n",
      ">418, d1=0.783, d2=0.729 g=0.772, a1=25, a2=37\n",
      ">419, d1=0.780, d2=0.752 g=0.769, a1=25, a2=32\n",
      ">420, d1=0.808, d2=0.743 g=0.773, a1=21, a2=34\n",
      ">421, d1=0.809, d2=0.728 g=0.746, a1=21, a2=43\n",
      ">422, d1=0.769, d2=0.768 g=0.733, a1=31, a2=39\n",
      ">423, d1=0.730, d2=0.731 g=0.744, a1=43, a2=48\n",
      ">424, d1=0.766, d2=0.814 g=0.748, a1=40, a2=15\n",
      ">425, d1=0.815, d2=0.753 g=0.700, a1=18, a2=35\n",
      ">426, d1=0.760, d2=0.736 g=0.722, a1=35, a2=31\n",
      ">427, d1=0.827, d2=0.769 g=0.719, a1=17, a2=23\n",
      ">428, d1=0.730, d2=0.726 g=0.727, a1=37, a2=50\n",
      ">429, d1=0.822, d2=0.760 g=0.702, a1=21, a2=35\n",
      ">430, d1=0.758, d2=0.763 g=0.712, a1=32, a2=32\n",
      ">431, d1=0.795, d2=0.772 g=0.696, a1=34, a2=32\n",
      ">432, d1=0.743, d2=0.747 g=0.711, a1=32, a2=31\n",
      ">433, d1=0.838, d2=0.786 g=0.711, a1=23, a2=28\n",
      ">434, d1=0.768, d2=0.828 g=0.695, a1=34, a2=18\n",
      ">435, d1=0.838, d2=0.787 g=0.720, a1=18, a2=23\n",
      ">436, d1=0.789, d2=0.809 g=0.722, a1=26, a2=20\n",
      ">437, d1=0.745, d2=0.763 g=0.724, a1=37, a2=34\n",
      ">438, d1=0.803, d2=0.775 g=0.724, a1=17, a2=21\n",
      ">439, d1=0.758, d2=0.769 g=0.733, a1=29, a2=26\n",
      ">440, d1=0.801, d2=0.690 g=0.714, a1=20, a2=53\n",
      ">441, d1=0.736, d2=0.803 g=0.708, a1=34, a2=20\n",
      ">442, d1=0.747, d2=0.776 g=0.730, a1=37, a2=21\n",
      ">443, d1=0.807, d2=0.759 g=0.725, a1=20, a2=25\n",
      ">444, d1=0.775, d2=0.741 g=0.728, a1=28, a2=39\n",
      ">445, d1=0.775, d2=0.797 g=0.739, a1=28, a2=17\n",
      ">446, d1=0.750, d2=0.771 g=0.737, a1=32, a2=31\n",
      ">447, d1=0.757, d2=0.741 g=0.750, a1=35, a2=39\n",
      ">448, d1=0.785, d2=0.697 g=0.742, a1=28, a2=51\n",
      ">449, d1=0.776, d2=0.779 g=0.745, a1=31, a2=32\n",
      ">450, d1=0.772, d2=0.784 g=0.733, a1=28, a2=29\n"
     ]
    }
   ],
   "source": [
    "# example of training a stable gan for generating a handwritten digit\n",
    "from os import makedirs\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(28,28,1)):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    # downsample to 14x14\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init,\n",
    "    input_shape=in_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample to 7x7\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    model.add(Dense(n_nodes, kernel_initializer=init, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    # upsample to 14x14\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same',\n",
    "                    kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 28x28\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same',\n",
    "                    kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # output 28x28x1\n",
    "    model.add(Conv2D(1, (7,7), activation='tanh', padding='same', kernel_initializer=init))\n",
    "    return model\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "# load mnist images\n",
    "def load_real_samples():\n",
    "    # load dataset\n",
    "    (trainX, trainy), (_, _) = load_data()\n",
    "    # expand to 3d, e.g. add channels\n",
    "    X = expand_dims(trainX, axis=-1)\n",
    "    # select all of the examples for a given class\n",
    "    selected_ix = trainy == 8\n",
    "    X = X[selected_ix]\n",
    "    # convert from ints to floats\n",
    "    X = X.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    return X\n",
    "\n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # select images\n",
    "    X = dataset[ix]\n",
    "    # generate class labels\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, latent_dim, n_samples=100):\n",
    "    # prepare fake examples\n",
    "    X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    X = (X + 1) / 2.0\n",
    "    # plot images\n",
    "    for i in range(10 * 10):\n",
    "        # define subplot\n",
    "        pyplot.subplot(10, 10, 1 + i)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
    "    # save plot to file\n",
    "    pyplot.savefig('results_baseline/generated_plot_%03d.png' % (step+1))\n",
    "    pyplot.close()\n",
    "    # save the generator model\n",
    "    g_model.save('results_baseline/model_%03d.h5' % (step+1))\n",
    "\n",
    "# create a line plot of loss for the gan and save to file\n",
    "def plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist):\n",
    "    # plot loss\n",
    "    pyplot.subplot(2, 1, 1)\n",
    "    pyplot.plot(d1_hist, label='d-real')\n",
    "    pyplot.plot(d2_hist, label='d-fake')\n",
    "    pyplot.plot(g_hist, label='gen')\n",
    "    pyplot.legend()\n",
    "    # plot discriminator accuracy\n",
    "    pyplot.subplot(2, 1, 2)\n",
    "    pyplot.plot(a1_hist, label='acc-real')\n",
    "    pyplot.plot(a2_hist, label='acc-fake')\n",
    "    pyplot.legend()\n",
    "    # save plot to file\n",
    "    pyplot.savefig('results_baseline/plot_line_plot_loss.png')\n",
    "    pyplot.close()\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=10, n_batch=128): # 10 128\n",
    "    # calculate the number of batches per epoch\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    # calculate the total iterations based on batch and epoch\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    # calculate the number of samples in half a batch\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # prepare lists for storing stats each iteration\n",
    "    d1_hist, d2_hist, g_hist, a1_hist, a2_hist = list(), list(), list(), list(), list()\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        # get randomly selected ✬real✬ samples\n",
    "        X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "        # update discriminator model weights\n",
    "        d_loss1, d_acc1 = d_model.train_on_batch(X_real, y_real)\n",
    "        # generate ✬fake✬ examples\n",
    "        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        # update discriminator model weights\n",
    "        d_loss2, d_acc2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "        # prepare points in latent space as input for the generator\n",
    "        X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = ones((n_batch, 1))\n",
    "        # update the generator via the discriminator✬s error\n",
    "        for j in range(2):\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "        # summarize loss on this batch\n",
    "        print('>%d, d1=%.3f, d2=%.3f g=%.3f, a1=%d, a2=%d' %\n",
    "                (i+1, d_loss1, d_loss2, g_loss, int(100*d_acc1), int(100*d_acc2)))\n",
    "        # record history\n",
    "        d1_hist.append(d_loss1)\n",
    "        d2_hist.append(d_loss2)\n",
    "        g_hist.append(g_loss)\n",
    "        a1_hist.append(d_acc1)\n",
    "        a2_hist.append(d_acc2)\n",
    "        # evaluate the model performance every ✬epoch✬\n",
    "        if (i+1) % bat_per_epo == 0:\n",
    "            summarize_performance(i, g_model, latent_dim)\n",
    "    plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist)\n",
    "    \n",
    "# make folder for results\n",
    "makedirs('results_baseline', exist_ok=True)\n",
    "# size of the latent space\n",
    "latent_dim = 50\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# load image data\n",
    "dataset = load_real_samples()\n",
    "print(dataset.shape)\n",
    "# train model\n",
    "train(generator, discriminator, gan_model, dataset, latent_dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
